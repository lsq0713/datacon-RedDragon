{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 黄牛检测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "file_path = './data/raw_data.xlsx'\n",
    "sheet_name = 'Sheet1'\n",
    "data = pd.read_excel(file_path, sheet_name=sheet_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 时间预处理\n",
    "data['订单创建时间'] = pd.to_datetime(data['订单创建时间'])\n",
    "data['就诊日期'] = pd.to_datetime(data['就诊日期'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ID', '省份', 'APPID', 'IP_ADDRESS', '订单创建时间', '患者ID', '患者创建时间', '就诊日期',\n",
      "       '就诊科室名称', '医生姓名', '状态', '商户订单号'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 写入到指定log文件中(LogBERT相关,已废弃)\n",
    "def write_log(log_file_path):\n",
    "    # 将数据逐行写入.log 文件\n",
    "    with open(log_file_path, 'w', encoding='utf-8') as log_file:\n",
    "        for index, row in data.iterrows():\n",
    "            # 将每一行转换为字符串并写入日志文件\n",
    "            row = row.to_dict()\n",
    "            log_file.write(row['患者ID'] + \" : \")\n",
    "            for k, v in row.items():\n",
    "                if k != \"患者ID\":\n",
    "                    log_file.write(str(v) + \" \")\n",
    "            log_file.write(\"\\n\")\n",
    "    print(f\"Data has been written to {log_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 基于规则的检测\n",
    "\n",
    "1. IP重复，来自同一个IP，且为超过3个人挂号\n",
    "2. 用户重复，来自同一个用户，且挂号了超过3个科室/超过两个app_id\n",
    "3. 时间过早，每天5:00-5:01进行操作的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重复值筛选，找出data的seg_name字段中重复数大于limit的行, sort_add和asc_add是检测完之后添加的排序要求\n",
    "def duplicate_detect(data, seg_name, limit, up=True, sort_add=[], asc_add=[]):\n",
    "    assert len(sort_add) == len(asc_add)\n",
    "    value_counts = data[seg_name].value_counts()\n",
    "    if up:\n",
    "        # 向上筛\n",
    "        dup_row = data[data[seg_name].isin(value_counts[value_counts > limit].index)]\n",
    "    else:\n",
    "        # 向下筛\n",
    "        dup_row = data[data[seg_name].isin(value_counts[value_counts < limit].index)]\n",
    "    sort_by = [seg_name] + sort_add\n",
    "    asc = [True] + asc_add\n",
    "    dup_row = dup_row.sort_values(by=sort_by, ascending=asc)\n",
    "    return dup_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 筛选出同一个dup_seg中有超过limit个unique_seg的ID字段\n",
    "def unique_dup_filter(data, dup_seg, unique_seg, limit, up=True, get_row=False):\n",
    "    # 确定是黄牛的行编号\n",
    "    selects = []\n",
    "    # IP重复检测\n",
    "    dup = duplicate_detect(data, dup_seg, limit, up=up, sort_add=[unique_seg], asc_add=[True])\n",
    "    print(\"init dup_num:\", len(dup))\n",
    "    if len(dup) == 0:\n",
    "        return selects\n",
    "    pos = 0          # 这个IP的起点\n",
    "    count = 1      # 涉及多少个用户ID\n",
    "    last = dup[unique_seg].iloc[0] # 上一个用户的ID\n",
    "    dup_num = len(dup)\n",
    "    for i in tqdm(range(1, dup_num)):\n",
    "        if dup[dup_seg].iloc[i] == dup[dup_seg].iloc[pos]:\n",
    "            if dup[unique_seg].iloc[i] != last:\n",
    "                # 相同IP下一个新的患者\n",
    "                count += 1\n",
    "                last = dup[unique_seg].iloc[i]\n",
    "        if dup[dup_seg].iloc[i] != dup[dup_seg].iloc[pos] or i == dup_num - 1:\n",
    "            # 开始检测下一个IP\n",
    "            if count > limit:\n",
    "                # 达到重复人数条件\n",
    "                for j in range(pos, i):\n",
    "                    selects.append(dup['ID'].iloc[j])\n",
    "            # 重置\n",
    "            pos = i\n",
    "            count = 1\n",
    "            last = dup[unique_seg].iloc[i]\n",
    "    print(\"filtered dup_num:\", len(selects))\n",
    "    if not get_row:\n",
    "        selects.sort()\n",
    "        return selects\n",
    "    else:\n",
    "        dup_rows = data[data['ID'].isin(selects)]\n",
    "        dup_rows = dup_rows.sort_values(by=dup_seg, ascending=True)\n",
    "        return dup_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对unique_dup_filter, 在一定范围内遍历limit(基本不用)\n",
    "# 使用: limits, select_list = grid_traverse(data, '患者ID', '就诊科室名称', 5, 10)\n",
    "def grid_traverse(data, dup_seg, unique_seg, start, end, gap=1):\n",
    "    limits = []\n",
    "    selects_list = []\n",
    "    for limit in range(start, end, gap):\n",
    "        print(f\"limit: {limit}\")\n",
    "        selects = unique_dup_filter(data, dup_seg, unique_seg, limit)\n",
    "        limits.append(limit)\n",
    "        selects_list.append(selects)\n",
    "    return limits, selects_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 时间段过滤器，过滤出每天一段时间内的数据\n",
    "# 形如daily_filter(data, '5:00:00', '5:01:00')\n",
    "def daily_filter(data, start, end):\n",
    "    return data[(data['订单创建时间'].dt.time >= pd.to_datetime(start).time()) &\n",
    "                    (data['订单创建时间'].dt.time <= pd.to_datetime(end).time())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 时间分段统计(数据分析而非处理, 只会打印出每个小时内各个操作的次数)\n",
    "def hour_count():\n",
    "    print(\"\\t\\t总数\\t已挂号  医保换号  已退号  窗口退号  无号退款  超时取消\")\n",
    "    for i in range(5, 23):\n",
    "        time_filter = data[(data['订单创建时间'].dt.time >= pd.to_datetime(f'{i}:00:00').time()) &\n",
    "                     (data['订单创建时间'].dt.time <= pd.to_datetime(f'{i+1}:00:00').time())]\n",
    "        counts = time_filter['状态'].value_counts()\n",
    "        print(f\"{i}:00 - {i+1}:00 \\t{len(time_filter)}\\t{counts['已挂号']}\\t{counts['医保换号']}\\t {counts['已退号']}\\t\"\n",
    "              f\"   {counts['窗口退号']}\\t   {counts['无号退款']}\\t    {counts['超时取消']}\\t\")\n",
    "\n",
    "# 一个小时内按分钟打印操作次数(数据分析而非处理)\n",
    "def minute_line(data, hour):\n",
    "    count = []\n",
    "    for i in tqdm(range(0, 59, 5)):\n",
    "        start = str(i)\n",
    "        end = str(i+5)\n",
    "        if len(start) < 2:\n",
    "            start = '0' + start\n",
    "        if len(end) < 2:\n",
    "            end = '0' + end \n",
    "        time_filter = data[(data['订单创建时间'].dt.time >= pd.to_datetime(f'{hour}:{start}:00').time()) &\n",
    "                     (data['订单创建时间'].dt.time <= pd.to_datetime(f'{hour}:{end}:00').time())]\n",
    "        count.append(len(time_filter))\n",
    "    plt.plot(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 恰好在16:00进行第二天/下一周操作(因为这里面很有可能有很多正常人,不建议使用)\n",
    "def hurry_sixteen(data, get_row=False):\n",
    "    gap = daily_filter(data, '16:00:00', '16:00:01')\n",
    "    time_diff = (gap['就诊日期'] - gap['订单创建时间']).dt.days\n",
    "    hurry_row = gap[(time_diff == 0) | (time_diff == 6)]\n",
    "    if get_row:\n",
    "        return hurry_row\n",
    "    else:\n",
    "        return hurry_row['ID'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将之前一次答案中的数据提取成列表(需要配套data文件夹中的result1/2/3...使用)\n",
    "def get_list(res_id, file_path=\"\"):\n",
    "    if file_path == \"\":\n",
    "        file_path = f'./data/result{res_id}.txt'\n",
    "    with open(file_path, 'r') as file:\n",
    "        lis = [int(line.strip()) for line in file]\n",
    "    return lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 两个list进行对比(相比增加的,减少的,相同的)\n",
    "def lis_cmp(lis1, lis2, ret=False, show=False):\n",
    "    set1 = set(lis1)\n",
    "    set2 = set(lis2)\n",
    "    new_ele = set2 - set1\n",
    "    miss_ele = set1 - set2\n",
    "    same_ele = set1.intersection(set2)\n",
    "    if show:\n",
    "        print(f\"lis1: {len(lis1)},\\tlis2: {len(lis2)},\\tmore: {len(new_ele)},\"\n",
    "            f\"\\tmiss: {len(miss_ele)},\\tsame = {len(same_ele)}\")\n",
    "    if ret:\n",
    "        return list(new_ele), list(miss_ele), list(same_ele)\n",
    "\n",
    "# 与之前一次答案进行对比\n",
    "def res_cmp(res_id, lis, ret=False, show=False):\n",
    "    lis1 = get_list(res_id)\n",
    "    res = lis_cmp(lis1, lis, ret, show)\n",
    "    if ret:\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将一个list写作答案(写到data/result.txt中,remove_white选项是是否去除白名单)\n",
    "def write_list(lis, remove_white=True):\n",
    "    if remove_white:\n",
    "        white_lis = get_list(0, './data/white.txt')\n",
    "        print(f\"remove white: {len(set(lis).intersection(set(white_lis)))}\")\n",
    "        lis = list(set(lis) - set(white_lis))\n",
    "        lis.sort()\n",
    "    # 打开一个文件进行写入，如果文件不存在则创建\n",
    "    with open('./data/result.txt', 'w', encoding='utf-8') as file:\n",
    "        # 遍历列表中的每个元素\n",
    "        for item in lis:\n",
    "            # 将每个元素写入文件，每个元素后面加上换行符\n",
    "            file.write(str(item) + '\\n')\n",
    "        print(f\"Total line: {len(lis)}\")\n",
    "    return lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data中退号超过limit词的(这条规则效果似乎不算太好)\n",
    "def frequent_drop(data, limit, get_row=False):\n",
    "    drop = data[data['状态'] == '已退号']\n",
    "    mass_drop = duplicate_detect(drop, \"患者ID\", limit)\n",
    "    if get_row:\n",
    "        return mass_drop['ID'].tolist()\n",
    "    else:\n",
    "        return mass_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简单筛选不是黄牛的用户(效果不确定)\n",
    "def normal_user(data):\n",
    "    # 黑名单\n",
    "    black_lis = get_list(6)\n",
    "    # 白名单\n",
    "    white_user1 = get_list(-1, file_path=\"./data/white.txt\")\n",
    "    black_rows = data[data['ID'].isin(black_lis)]\n",
    "    black_user = black_rows['患者ID'].to_list()\n",
    "    not_black = data[~data['患者ID'].isin(black_user)]['患者ID'].to_list()\n",
    "    white_sample = random.sample(not_black, 10 * len(white_user1))\n",
    "    white_user2 = data[data['患者ID'].isin(white_sample)]['ID'].to_list()\n",
    "    white_ids = list(set(white_user1 + white_user2))\n",
    "    white_rows = data[data['ID'].isin(white_ids)]\n",
    "    return white_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据权重在多个不同的list中进行合并筛选\n",
    "def weighted_selection(lists, weights, n, limit=0):\n",
    "    # 创建一个字典来存储元素的总权重\n",
    "    total_weights = defaultdict(float)\n",
    "\n",
    "    # 遍历每个列表及其对应的权重\n",
    "    for lst, weight in zip(lists, weights):\n",
    "        for element in lst:\n",
    "            total_weights[element] += weight  # 累加权重\n",
    "\n",
    "    # 将字典转换为列表，并筛选出权重大于 limit 的元素\n",
    "    filtered_elements = {k: v for k, v in total_weights.items() if v >= limit}\n",
    "\n",
    "    # 按照权重排序\n",
    "    sorted_elements = sorted(filtered_elements.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # 选择前 n 个元素及其权重\n",
    "    top_n_elements = sorted_elements[:n]\n",
    "\n",
    "    return top_n_elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 基于聚类的分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ip_encoder(ip):\n",
    "    parts = list(map(int, ip.split('.')))\n",
    "    return (parts[0] << 24) + (parts[1] << 16) + (parts[2] << 8) + parts[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 基于学习的分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "state2num = {\n",
    "    '已退号': 0,\n",
    "    '已挂号': 1,\n",
    "    '窗口退号': 2,\n",
    "    '医保换号': 3,\n",
    "    '无号退款': 4,\n",
    "    '超时取消': 5,\n",
    "    '主动取消': 6,\n",
    "    np.nan: 7\n",
    "}\n",
    "\n",
    "def get_embed_list(state_list_pd, seg_name, series_name):\n",
    "    state_lists = state_list_pd.groupby(seg_name)[series_name].apply(list).reset_index()\n",
    "    state_lists = state_lists[series_name].to_list()\n",
    "    for i in range(len(state_lists)):\n",
    "        state_lists[i] = [str(state2num[str_]) for str_ in state_lists[i]]\n",
    "    return state_lists\n",
    "\n",
    "def generate_train_test(data, seg_name, series_name, normal_func, limit=2000, path='../LogBERT/output/rd/'):\n",
    "    # train\n",
    "    normal_rows = normal_func(data)\n",
    "    normal_id = normal_rows[seg_name].to_list()\n",
    "    sample_id = random.sample(normal_id, limit)\n",
    "    normals = data[data[seg_name].isin(sample_id)]\n",
    "    state_lists = get_embed_list(normals, seg_name, series_name)\n",
    "    with open(path + \"train\", 'w') as train_file:\n",
    "        for states in state_lists:\n",
    "            train_file.write(\" \".join(states) + \"\\n\")\n",
    "\n",
    "    # test_normal\n",
    "    unsample_id = list(set(normal_id) - set(sample_id))\n",
    "    test_normals = data[data[seg_name].isin(unsample_id)]\n",
    "    state_lists = get_embed_list(test_normals, seg_name, series_name)\n",
    "    with open(path + \"test_normal\", 'w') as train_file:\n",
    "        for states in state_lists:\n",
    "            train_file.write(\" \".join(states) + \"\\n\")\n",
    "\n",
    "    # test_abnormal\n",
    "    res_id = get_list(5)\n",
    "    test_abnormals = data[data['ID'].isin(res_id)]\n",
    "    state_lists = get_embed_list(test_abnormals, seg_name, series_name)\n",
    "    with open(path + \"test_abnormal\", 'w') as train_file:\n",
    "        for states in state_lists:\n",
    "            train_file.write(\" \".join(states) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LogBERT 魔改记录\n",
    "\n",
    "1. logbert.py里的options[\"min_len\"]改成了1, epoch也改成了2\n",
    "2. sample.py中line = line.squeeze()改为line.ravel()\n",
    "3. sample.py中logkey_seq_pairs = np.array(logkey_seq_pairs,)加上了dtype=object，predict_log.py中如果报错也添加这个就行\n",
    "4. train_log.py中epoch > 0及生成center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将一个list写入白名单\n",
    "def add_white(lis, path='./data/white.txt'):\n",
    "    former_list = []\n",
    "    if os.path.isfile(path):\n",
    "        former_list = get_list(0, path)\n",
    "    lis = list(set(lis + former_list))\n",
    "    with open(path, 'w', encoding='utf-8') as file:\n",
    "        for item in lis:\n",
    "            file.write(str(item) + '\\n')\n",
    "        print(f\"Total line: {len(lis)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 省份, 患者ID, 就诊科室名称, 医生姓名, 状态\n",
    "\n",
    "# 按照用户分组提取出操作的时间序列，然后嵌入成一个torch.tensor\n",
    "def feature_extract(rows):\n",
    "    res_list = []\n",
    "    user_ids = []\n",
    "    # 根据User分成dataFrame的list\n",
    "    user_group = rows.groupby('患者ID')\n",
    "    data_list = [group.reset_index(drop=True) for name, group in user_group]\n",
    "    \n",
    "    # 每一组根据时间进行排序\n",
    "    for i in range(len(data_list)):\n",
    "        data_list[i] = data_list[i].sort_values(by='就诊日期', ascending=True)\n",
    "        \n",
    "        prov_tensor = torch.tensor(data_list[i]['prov_id'].values).unsqueeze(0)\n",
    "        depart_tensor = torch.tensor(data_list[i]['depart_id'].values).unsqueeze(0)\n",
    "        doc_tensor = torch.tensor(data_list[i]['doc_id'].values).unsqueeze(0)\n",
    "        state_tensor = torch.tensor(data_list[i]['state_id'].values).unsqueeze(0)\n",
    "        embedding = torch.cat((prov_tensor, depart_tensor, doc_tensor, state_tensor), dim=0).T\n",
    "        res_list.append(embedding)\n",
    "        user_ids.append(data_list[i]['患者ID'][0])\n",
    "    \n",
    "    return res_list, user_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将tensor填充成指定行数\n",
    "def padding(tensor, target_length):\n",
    "    current_length = tensor.size(0)  # 当前行数\n",
    "    padding_amount = target_length - current_length\n",
    "    if padding_amount > 0:\n",
    "        padding_tensor = torch.zeros(padding_amount, tensor.size(1), dtype=tensor.dtype)\n",
    "        padded_tensor = torch.cat((tensor, padding_tensor), dim=0)\n",
    "    else:\n",
    "        # 如果当前行数已经大于或等于目标长度，直接使用原 tensor\n",
    "        padded_tensor = tensor[:target_length]\n",
    "    return padded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_user_statistic(data):\n",
    "    id_counts = data.groupby('患者ID').size()\n",
    "    for i in range(0, 20):\n",
    "        filtered_ids = id_counts[id_counts > i]\n",
    "        result_count = len(filtered_ids)\n",
    "        print(f'操作数大于 {i} 的 患者 个数: {result_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_loader(samples, labels, SequenceDataset, train_ratio):\n",
    "    samples_list = list(samples)\n",
    "    labels_list = list(labels)\n",
    "\n",
    "    # 设置划分比例\n",
    "    train_size = int(train_ratio * len(samples_list))\n",
    "    indices = np.random.permutation(len(samples_list))  # 随机打乱索引\n",
    "\n",
    "    # 划分索引\n",
    "    train_indices = indices[:train_size]\n",
    "    test_indices = indices[train_size:]\n",
    "\n",
    "    # 创建训练和测试数据集\n",
    "    train_samples = [samples_list[i] for i in train_indices]\n",
    "    train_labels = [labels_list[i] for i in train_indices]\n",
    "\n",
    "    test_samples = [samples_list[i] for i in test_indices]\n",
    "    test_labels = [labels_list[i] for i in test_indices]\n",
    "\n",
    "    train_dataset = SequenceDataset(train_samples, train_labels)\n",
    "    test_dataset = SequenceDataset(test_samples, test_labels)\n",
    "\n",
    "    # 创建 DataLoader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "# 先对类别列进行编码\n",
    "data = data.fillna(method='ffill')     # 前值替换缺失值\n",
    "data['prov_id'] = pd.factorize(data['省份'])[0]\n",
    "data['depart_id'] = pd.factorize(data['就诊科室名称'])[0]\n",
    "data['doc_id'] = pd.factorize(data['医生姓名'])[0]\n",
    "data['state_id'] = pd.factorize(data['状态'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备数据\n",
    "## 黄牛\n",
    "abnormal_id = get_list(6)\n",
    "abnormal_rows = data[data['ID'].isin(abnormal_id)]\n",
    "## 正常人\n",
    "normal_rows = normal_user(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 黄牛特征向量\n",
    "abnormal_tensors, _ = feature_extract(abnormal_rows)\n",
    "# 普通人的特征向量\n",
    "normal_tensors, _ = feature_extract(normal_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建数据集\n",
    "## 标签数据\n",
    "normal_labels = torch.zeros(len(normal_tensors), dtype=torch.long)\n",
    "abnormal_labels = torch.ones(len(abnormal_tensors), dtype=torch.long)\n",
    "samples = normal_tensors + abnormal_tensors\n",
    "# 序列填充\n",
    "for i in range(len(samples)):\n",
    "    samples[i] = padding(samples[i], 15)\n",
    "labels = torch.cat((normal_labels, abnormal_labels))\n",
    "\n",
    "## 数据集\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, samples, labels):\n",
    "        self.samples = samples\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx], self.labels[idx]\n",
    "## 划分训练与测试\n",
    "train_loader, test_loader = train_test_loader(samples, labels, SequenceDataset, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型定义\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, model_dim, num_heads, num_classes, num_layers):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Linear(input_dim, model_dim)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(model_dim, num_heads),\n",
    "            num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(model_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_length, input_dim)\n",
    "        x = self.embedding(x)  # (batch_size, seq_length, model_dim)\n",
    "        x = x.transpose(0, 1)  # (seq_length, batch_size, model_dim) for Transformer\n",
    "        x = self.transformer_encoder(x)  # (seq_length, batch_size, model_dim)\n",
    "        x = x.mean(dim=0)  # Global average pooling\n",
    "        x = self.fc(x)  # (batch_size, num_classes)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Library\\Python\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "# 超参数设置\n",
    "# 超参数\n",
    "input_dim = 4  # 每个时间步的特征数量\n",
    "model_dim = 64  # Transformer 模型维度\n",
    "num_heads = 4  # 注意力头数量\n",
    "num_classes = 2  # 分类数量（正常和异常）\n",
    "num_layers = 2  # Transformer 层数\n",
    "\n",
    "# 创建模型，损失函数和优化器\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "model = TransformerModel(input_dim, model_dim, num_heads, num_classes, num_layers).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.0638\n",
      "Epoch [2/10], Loss: 0.0061\n",
      "Epoch [3/10], Loss: 0.0025\n",
      "Epoch [4/10], Loss: 0.0022\n",
      "Epoch [5/10], Loss: 0.0012\n",
      "Epoch [6/10], Loss: 0.0185\n",
      "Epoch [7/10], Loss: 1.5877\n",
      "Epoch [8/10], Loss: 0.0016\n",
      "Epoch [9/10], Loss: 0.0017\n",
      "Epoch [10/10], Loss: 0.0023\n"
     ]
    }
   ],
   "source": [
    "# 模型训练\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_samples, batch_labels in train_loader:\n",
    "        batch_samples = batch_samples.to(device).float()\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_samples)  # 前向传播\n",
    "        loss = criterion(outputs, batch_labels)  # 计算损失\n",
    "        loss.backward()  # 反向传播\n",
    "        optimizer.step()  # 优化器更新\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.03%\n"
     ]
    }
   ],
   "source": [
    "# 模型评估\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_samples, batch_labels in test_loader:\n",
    "        batch_samples = batch_samples.to(device).float()\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        outputs = model(batch_samples)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += batch_labels.size(0)\n",
    "        correct += (predicted == batch_labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实际使用\n",
    "model.eval()\n",
    "all_tensors, users = feature_extract(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225429/225429 [06:08<00:00, 612.32it/s] \n"
     ]
    }
   ],
   "source": [
    "classify = []\n",
    "for i in tqdm(range(len(all_tensors))):\n",
    "    all_tensors[i] = padding(all_tensors[i], 15)\n",
    "    input = all_tensors[i].to(device).float().unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        output = model(input)\n",
    "    predicted_class = torch.argmax(output, dim=1)\n",
    "    classify.append(predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [i for i, value in enumerate(classify) if value == 1]\n",
    "abnormal_uids = [users[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_detect_ids = data[data['患者ID'].isin(abnormal_uids)]['ID'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 工作区"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove white: 1290\n",
      "Total line: 9202\n"
     ]
    }
   ],
   "source": [
    "# 第十次测试(11.20)\n",
    "lis6 = get_list(6)\n",
    "model_more = list(set(model_detect_ids) - set(lis6))\n",
    "model_use = random.sample(model_more, min(len(model_more), 15000 - len(lis6)))\n",
    "select = list(set(model_use + lis6))\n",
    "torch.save(model.state_dict(), './data/result_train10.pth')\n",
    "select = write_list(select)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据分析结论\n",
    "\n",
    "1. 从第一次提交到第3次，涨幅均是来自于发现了科室限制规则中的其他数据，也就是说，其间新增的规则无效"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
