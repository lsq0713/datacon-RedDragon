{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 黄牛检测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "file_path = './data/raw_data.xlsx'\n",
    "sheet_name = 'Sheet1'\n",
    "data = pd.read_excel(file_path, sheet_name=sheet_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 时间预处理\n",
    "data['订单创建时间'] = pd.to_datetime(data['订单创建时间'])\n",
    "data['就诊日期'] = pd.to_datetime(data['就诊日期'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 写入到指定log文件中(LogBERT相关,已废弃)\n",
    "def write_log(log_file_path):\n",
    "    # 将数据逐行写入.log 文件\n",
    "    with open(log_file_path, 'w', encoding='utf-8') as log_file:\n",
    "        for index, row in data.iterrows():\n",
    "            # 将每一行转换为字符串并写入日志文件\n",
    "            row = row.to_dict()\n",
    "            log_file.write(row['患者ID'] + \" : \")\n",
    "            for k, v in row.items():\n",
    "                if k != \"患者ID\":\n",
    "                    log_file.write(str(v) + \" \")\n",
    "            log_file.write(\"\\n\")\n",
    "    print(f\"Data has been written to {log_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 基于规则的检测\n",
    "\n",
    "1. IP重复，来自同一个IP，且为超过3个人挂号\n",
    "2. 用户重复，来自同一个用户，且挂号了超过3个科室/超过两个app_id\n",
    "3. 时间过早，每天5:00-5:01进行操作的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重复值筛选，找出data的seg_name字段中重复数大于limit的行, sort_add和asc_add是检测完之后添加的排序要求\n",
    "def duplicate_detect(data, seg_name, limit, up=True, sort_add=[], asc_add=[]):\n",
    "    assert len(sort_add) == len(asc_add)\n",
    "    value_counts = data[seg_name].value_counts()\n",
    "    if up:\n",
    "        # 向上筛\n",
    "        dup_row = data[data[seg_name].isin(value_counts[value_counts > limit].index)]\n",
    "    else:\n",
    "        # 向下筛\n",
    "        dup_row = data[data[seg_name].isin(value_counts[value_counts < limit].index)]\n",
    "    sort_by = [seg_name] + sort_add\n",
    "    asc = [True] + asc_add\n",
    "    dup_row = dup_row.sort_values(by=sort_by, ascending=asc)\n",
    "    return dup_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 筛选出同一个dup_seg中有超过limit个unique_seg的ID字段\n",
    "def unique_dup_filter(data, dup_seg, unique_seg, limit, up=True, get_row=False):\n",
    "    # 确定是黄牛的行编号\n",
    "    selects = []\n",
    "    # IP重复检测\n",
    "    dup = duplicate_detect(data, dup_seg, limit, up=up, sort_add=[unique_seg], asc_add=[True])\n",
    "    print(\"init dup_num:\", len(dup))\n",
    "    if len(dup) == 0:\n",
    "        return selects\n",
    "    pos = 0          # 这个IP的起点\n",
    "    count = 1      # 涉及多少个用户ID\n",
    "    last = dup[unique_seg].iloc[0] # 上一个用户的ID\n",
    "    dup_num = len(dup)\n",
    "    for i in tqdm(range(1, dup_num)):\n",
    "        if dup[dup_seg].iloc[i] == dup[dup_seg].iloc[pos]:\n",
    "            if dup[unique_seg].iloc[i] != last:\n",
    "                # 相同IP下一个新的患者\n",
    "                count += 1\n",
    "                last = dup[unique_seg].iloc[i]\n",
    "        if dup[dup_seg].iloc[i] != dup[dup_seg].iloc[pos] or i == dup_num - 1:\n",
    "            # 开始检测下一个IP\n",
    "            if count > limit:\n",
    "                # 达到重复人数条件\n",
    "                for j in range(pos, i):\n",
    "                    selects.append(dup['ID'].iloc[j])\n",
    "            # 重置\n",
    "            pos = i\n",
    "            count = 1\n",
    "            last = dup[unique_seg].iloc[i]\n",
    "    print(\"filtered dup_num:\", len(selects))\n",
    "    if not get_row:\n",
    "        selects.sort()\n",
    "        return selects\n",
    "    else:\n",
    "        dup_rows = data[data['ID'].isin(selects)]\n",
    "        dup_rows = dup_rows.sort_values(by=dup_seg, ascending=True)\n",
    "        return dup_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对unique_dup_filter, 在一定范围内遍历limit(基本不用)\n",
    "# 使用: limits, select_list = grid_traverse(data, '患者ID', '就诊科室名称', 5, 10)\n",
    "def grid_traverse(data, dup_seg, unique_seg, start, end, gap=1):\n",
    "    limits = []\n",
    "    selects_list = []\n",
    "    for limit in range(start, end, gap):\n",
    "        print(f\"limit: {limit}\")\n",
    "        selects = unique_dup_filter(data, dup_seg, unique_seg, limit)\n",
    "        limits.append(limit)\n",
    "        selects_list.append(selects)\n",
    "    return limits, selects_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 时间段过滤器，过滤出每天一段时间内的数据\n",
    "# 形如daily_filter(data, '5:00:00', '5:01:00')\n",
    "def daily_filter(data, start, end):\n",
    "    return data[(data['订单创建时间'].dt.time >= pd.to_datetime(start).time()) &\n",
    "                    (data['订单创建时间'].dt.time <= pd.to_datetime(end).time())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 时间分段统计(数据分析而非处理, 只会打印出每个小时内各个操作的次数)\n",
    "def hour_count():\n",
    "    print(\"\\t\\t总数\\t已挂号  医保换号  已退号  窗口退号  无号退款  超时取消\")\n",
    "    for i in range(5, 23):\n",
    "        time_filter = data[(data['订单创建时间'].dt.time >= pd.to_datetime(f'{i}:00:00').time()) &\n",
    "                     (data['订单创建时间'].dt.time <= pd.to_datetime(f'{i+1}:00:00').time())]\n",
    "        counts = time_filter['状态'].value_counts()\n",
    "        print(f\"{i}:00 - {i+1}:00 \\t{len(time_filter)}\\t{counts['已挂号']}\\t{counts['医保换号']}\\t {counts['已退号']}\\t\"\n",
    "              f\"   {counts['窗口退号']}\\t   {counts['无号退款']}\\t    {counts['超时取消']}\\t\")\n",
    "\n",
    "# 一个小时内按分钟打印操作次数(数据分析而非处理)\n",
    "def minute_line(data, hour):\n",
    "    count = []\n",
    "    for i in tqdm(range(0, 59, 5)):\n",
    "        start = str(i)\n",
    "        end = str(i+5)\n",
    "        if len(start) < 2:\n",
    "            start = '0' + start\n",
    "        if len(end) < 2:\n",
    "            end = '0' + end \n",
    "        time_filter = data[(data['订单创建时间'].dt.time >= pd.to_datetime(f'{hour}:{start}:00').time()) &\n",
    "                     (data['订单创建时间'].dt.time <= pd.to_datetime(f'{hour}:{end}:00').time())]\n",
    "        count.append(len(time_filter))\n",
    "    plt.plot(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 恰好在16:00进行第二天/下一周操作(因为这里面很有可能有很多正常人,不建议使用)\n",
    "def hurry_sixteen(data, get_row=False):\n",
    "    gap = daily_filter(data, '16:00:00', '16:00:01')\n",
    "    time_diff = (gap['就诊日期'] - gap['订单创建时间']).dt.days\n",
    "    hurry_row = gap[(time_diff == 0) | (time_diff == 6)]\n",
    "    if get_row:\n",
    "        return hurry_row\n",
    "    else:\n",
    "        return hurry_row['ID'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将之前一次答案中的数据提取成列表(需要配套data文件夹中的result1/2/3...使用)\n",
    "def get_list(res_id, file_path=\"\"):\n",
    "    if file_path == \"\":\n",
    "        file_path = f'./data/result{res_id}.txt'\n",
    "    with open(file_path, 'r') as file:\n",
    "        lis = [int(line.strip()) for line in file]\n",
    "    return lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 两个list进行对比(相比增加的,减少的,相同的)\n",
    "def lis_cmp(lis1, lis2, ret=False, show=False):\n",
    "    set1 = set(lis1)\n",
    "    set2 = set(lis2)\n",
    "    new_ele = set2 - set1\n",
    "    miss_ele = set1 - set2\n",
    "    same_ele = set1.intersection(set2)\n",
    "    if show:\n",
    "        print(f\"lis1: {len(lis1)},\\tlis2: {len(lis2)},\\tmore: {len(new_ele)},\"\n",
    "            f\"\\tmiss: {len(miss_ele)},\\tsame = {len(same_ele)}\")\n",
    "    if ret:\n",
    "        return list(new_ele), list(miss_ele), list(same_ele)\n",
    "\n",
    "# 与之前一次答案进行对比\n",
    "def res_cmp(res_id, lis, ret=False, show=False):\n",
    "    lis1 = get_list(res_id)\n",
    "    res = lis_cmp(lis1, lis, ret, show)\n",
    "    if ret:\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将一个list写作答案(写到data/result.txt中,remove_white选项是是否去除白名单)\n",
    "def write_list(lis, remove_white=True):\n",
    "    if remove_white:\n",
    "        white_lis = get_list(0, './data/white.txt')\n",
    "        lis = list(set(lis) - set(white_lis))\n",
    "    # 打开一个文件进行写入，如果文件不存在则创建\n",
    "    with open('./data/result.txt', 'w', encoding='utf-8') as file:\n",
    "        # 遍历列表中的每个元素\n",
    "        for item in lis:\n",
    "            # 将每个元素写入文件，每个元素后面加上换行符\n",
    "            file.write(str(item) + '\\n')\n",
    "        print(f\"Total line: {len(lis)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data中退号超过limit词的(这条规则效果似乎不算太好)\n",
    "def frequent_drop(data, limit, get_row=False):\n",
    "    drop = data[data['状态'] == '已退号']\n",
    "    mass_drop = duplicate_detect(drop, \"患者ID\", limit)\n",
    "    if get_row:\n",
    "        return mass_drop['ID'].tolist()\n",
    "    else:\n",
    "        return mass_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简单筛选不是黄牛的用户(效果不确定)\n",
    "def normal_user(data):\n",
    "    # 黑名单\n",
    "    ip_black = duplicate_detect(data, 'IP_ADDRESS', 2)['IP_ADDRESS'].to_list()\n",
    "    user_black = duplicate_detect(data, '患者ID', 4)['患者ID'].to_list()\n",
    "    # 删除黑名单中对应的所有ip和患者ID\n",
    "    white_rows = data[~data['IP_ADDRESS'].isin(ip_black)]\n",
    "    white_rows = white_rows[~data['患者ID'].isin(user_black)]\n",
    "    return white_rows\n",
    "    # 基于用户分组的进一步筛选\n",
    "    # depart_limit = white_rows.groupby('患者ID').filter(lambda group: group['就诊科室名称'].nunique() <= 2)\n",
    "    # app_limit = depart_limit.groupby('患者ID').filter(lambda group: group['APPID'].nunique() <= 2)\n",
    "    # area_limit = app_limit[(app_limit['省份'] == '北京') | (app_limit['省份'] == '河北')]\n",
    "    # time_limit = area_limit[((area_limit['订单创建时间'].dt.time >= pd.to_datetime(f'6:00:00').time()) &\n",
    "    #                  (area_limit['订单创建时间'].dt.time <= pd.to_datetime(f'16:00:00').time())) | \n",
    "    #                  (area_limit['订单创建时间'].dt.time >= pd.to_datetime(f'17:00:00').time())]\n",
    "    # return time_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据权重在多个不同的list中进行合并筛选\n",
    "def weighted_selection(lists, weights, n, limit=0):\n",
    "    # 创建一个字典来存储元素的总权重\n",
    "    total_weights = defaultdict(float)\n",
    "\n",
    "    # 遍历每个列表及其对应的权重\n",
    "    for lst, weight in zip(lists, weights):\n",
    "        for element in lst:\n",
    "            total_weights[element] += weight  # 累加权重\n",
    "\n",
    "    # 将字典转换为列表，并筛选出权重大于 limit 的元素\n",
    "    filtered_elements = {k: v for k, v in total_weights.items() if v >= limit}\n",
    "\n",
    "    # 按照权重排序\n",
    "    sorted_elements = sorted(filtered_elements.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # 选择前 n 个元素及其权重\n",
    "    top_n_elements = sorted_elements[:n]\n",
    "\n",
    "    return top_n_elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 基于聚类的分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ip_encoder(ip):\n",
    "    parts = list(map(int, ip.split('.')))\n",
    "    return (parts[0] << 24) + (parts[1] << 16) + (parts[2] << 8) + parts[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 基于学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "state2num = {\n",
    "    '已退号': 0,\n",
    "    '已挂号': 1,\n",
    "    '窗口退号': 2,\n",
    "    '医保换号': 3,\n",
    "    '无号退款': 4,\n",
    "    '超时取消': 5,\n",
    "    '主动取消': 6,\n",
    "    np.nan: 7\n",
    "}\n",
    "\n",
    "def get_embed_list(state_list_pd, seg_name, series_name):\n",
    "    state_lists = state_list_pd.groupby(seg_name)[series_name].apply(list).reset_index()\n",
    "    state_lists = state_lists[series_name].to_list()\n",
    "    for i in range(len(state_lists)):\n",
    "        state_lists[i] = [str(state2num[str_]) for str_ in state_lists[i]]\n",
    "    return state_lists\n",
    "\n",
    "def generate_train_test(data, seg_name, series_name, normal_func, limit=2000, path='../LogBERT/output/rd/'):\n",
    "    # train\n",
    "    normal_rows = normal_func(data)\n",
    "    normal_id = normal_rows[seg_name].to_list()\n",
    "    sample_id = random.sample(normal_id, limit)\n",
    "    normals = data[data[seg_name].isin(sample_id)]\n",
    "    state_lists = get_embed_list(normals, seg_name, series_name)\n",
    "    with open(path + \"train\", 'w') as train_file:\n",
    "        for states in state_lists:\n",
    "            train_file.write(\" \".join(states) + \"\\n\")\n",
    "\n",
    "    # test_normal\n",
    "    unsample_id = list(set(normal_id) - set(sample_id))\n",
    "    test_normals = data[data[seg_name].isin(unsample_id)]\n",
    "    state_lists = get_embed_list(test_normals, seg_name, series_name)\n",
    "    with open(path + \"test_normal\", 'w') as train_file:\n",
    "        for states in state_lists:\n",
    "            train_file.write(\" \".join(states) + \"\\n\")\n",
    "\n",
    "    # test_abnormal\n",
    "    res_id = get_list(5)\n",
    "    test_abnormals = data[data['ID'].isin(res_id)]\n",
    "    state_lists = get_embed_list(test_abnormals, seg_name, series_name)\n",
    "    with open(path + \"test_abnormal\", 'w') as train_file:\n",
    "        for states in state_lists:\n",
    "            train_file.write(\" \".join(states) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LogBERT 魔改记录\n",
    "\n",
    "1. logbert.py里的options[\"min_len\"]改成了1, epoch也改成了2\n",
    "2. sample.py中line = line.squeeze()改为line.ravel()\n",
    "3. sample.py中logkey_seq_pairs = np.array(logkey_seq_pairs,)加上了dtype=object，predict_log.py中如果报错也添加这个就行\n",
    "4. train_log.py中epoch > 0及生成center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将一个list写入白名单\n",
    "def add_white(lis, path='./data/white.txt'):\n",
    "    former_list = []\n",
    "    if os.path.isfile(path):\n",
    "        former_list = get_list(0, path)\n",
    "    lis = list(set(lis + former_list))\n",
    "    with open(path, 'w', encoding='utf-8') as file:\n",
    "        for item in lis:\n",
    "            file.write(str(item) + '\\n')\n",
    "        print(f\"Total line: {len(lis)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lis1: 6339,\tlis2: 9599,\tmore: 4881,\tmiss: 1621,\tsame = 4718\n",
      "lis1: 8480,\tlis2: 9599,\tmore: 4556,\tmiss: 3437,\tsame = 5043\n",
      "lis1: 5824,\tlis2: 9599,\tmore: 5712,\tmiss: 1937,\tsame = 3887\n",
      "lis1: 8921,\tlis2: 9599,\tmore: 4145,\tmiss: 3467,\tsame = 5454\n",
      "lis1: 3210,\tlis2: 9599,\tmore: 6728,\tmiss: 339,\tsame = 2871\n",
      "lis1: 8352,\tlis2: 9599,\tmore: 2706,\tmiss: 1459,\tsame = 6893\n",
      "lis1: 9696,\tlis2: 9599,\tmore: 2521,\tmiss: 2618,\tsame = 7078\n"
     ]
    }
   ],
   "source": [
    "lis = get_list(8)\n",
    "for i in range(1, 8):\n",
    "    res_cmp(i, lis, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 工作区"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 省份, 患者ID, 就诊科室名称, 医生姓名, 状态\n",
    "\n",
    "# 按照用户分组提取出操作的时间序列，然后嵌入成一个torch.tensor\n",
    "def feature_extract(rows, prov_embedder, depart_embedder, doc_embedder, state_embedder):\n",
    "    res_list = []\n",
    "    # 根据User分成dataFrame的list\n",
    "    user_group = rows.groupby('患者ID')\n",
    "    data_list = [group.reset_index(drop=True) for name, group in user_group]\n",
    "    \n",
    "    # 每一组根据时间进行排序\n",
    "    for i in range(len(data_list)):\n",
    "        data_list[i] = data_list[i].sort_values(by='就诊日期', ascending=True)\n",
    "        \n",
    "        prov_tensor = torch.tensor(rows['prov_id'].values)\n",
    "        depart_tensor = torch.tensor(rows['depart_id'].values)\n",
    "        doc_tensor = torch.tensor(rows['doc_id'].values)\n",
    "        state_tensor = torch.tensor(rows['state_id'].values)\n",
    "\n",
    "        prov_tensor = prov_embedder(prov_tensor)\n",
    "        depart_tensor = depart_embedder(depart_tensor)\n",
    "        doc_tensor = doc_embedder(doc_tensor)\n",
    "        state_tensor = state_embedder(state_tensor)\n",
    "\n",
    "        embedding = torch.cat((prov_tensor, depart_tensor, doc_tensor, state_tensor), dim=1)\n",
    "\n",
    "        res_list.append(embedding)\n",
    "        break\n",
    "    \n",
    "    return res_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "# 先对类别列进行编码\n",
    "data['prov_id'] = pd.factorize(data['省份'])[0]\n",
    "data['depart_id'] = pd.factorize(data['就诊科室名称'])[0]\n",
    "data['doc_id'] = pd.factorize(data['医生姓名'])[0]\n",
    "data['state_id'] = pd.factorize(data['状态'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备数据\n",
    "## 黄牛\n",
    "abnormal_id = get_list(6)\n",
    "abnormal_rows = data[data['ID'].isin(abnormal_id)]\n",
    "## 正常人"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征提取\n",
    "# 创建embedding\n",
    "PROV_DIM = 1\n",
    "DEPART_DIM = 2\n",
    "DOC_DIM = 1\n",
    "STATE_DIM = 2\n",
    "prov_embbeder = nn.Embedding(max(data['prov_id']) + 1, PROV_DIM)\n",
    "depart_embbeder = nn.Embedding(max(data['depart_id']) + 1, DEPART_DIM)\n",
    "doc_embbeder = nn.Embedding(max(data['doc_id']) + 1, DOC_DIM)\n",
    "state_embbeder = nn.Embedding(max(data['state_id']) + 1, STATE_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[147], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mfeature_extract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mabnormal_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprov_embbeder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepart_embbeder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc_embbeder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_embbeder\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[143], line 19\u001b[0m, in \u001b[0;36mfeature_extract\u001b[1;34m(rows, prov_embedder, depart_embedder, doc_embedder, state_embedder)\u001b[0m\n\u001b[0;32m     16\u001b[0m doc_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(rows[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoc_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[0;32m     17\u001b[0m state_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(rows[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[1;32m---> 19\u001b[0m prov_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mprov_embedder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprov_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m depart_tensor \u001b[38;5;241m=\u001b[39m depart_embedder(depart_tensor)\n\u001b[0;32m     21\u001b[0m doc_tensor \u001b[38;5;241m=\u001b[39m doc_embedder(doc_tensor)\n",
      "File \u001b[1;32mc:\\Library\\Python\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Library\\Python\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Library\\Python\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:164\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Library\\Python\\anaconda\\lib\\site-packages\\torch\\nn\\functional.py:2267\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2261\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2262\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2263\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2264\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2265\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2266\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "feature_extract(abnormal_rows, prov_embbeder, depart_embbeder, doc_embbeder, state_embbeder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型输出形状: torch.Size([3, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Library\\Python\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "# 模型定义\n",
    "class TransLog(nn.Module):\n",
    "    def __init__(self, input_dim, model_dim, n_heads, num_classes, num_layers, dropout=0.1):\n",
    "        super(TransLog, self).__init__()\n",
    "        \n",
    "        # 输入嵌入层\n",
    "        self.embedding = nn.Linear(input_dim, model_dim)\n",
    "        \n",
    "        # Dropout层\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Transformer 编码器\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=model_dim, nhead=n_heads, dropout=dropout),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        # 分类层\n",
    "        self.fc = nn.Linear(model_dim, num_classes)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # 输入形状: (seq_len, batch_size, input_dim)\n",
    "        x = self.embedding(x)  # (seq_len, batch_size, model_dim)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # 通过 Transformer 编码器\n",
    "        x = self.transformer_encoder(x)  # (seq_len, batch_size, model_dim)\n",
    "        \n",
    "        # 取最后一个时间步的输出\n",
    "        x = x[-1, :, :]  # (batch_size, model_dim)\n",
    "        \n",
    "        # 分类\n",
    "        x = self.fc(x)  # (batch_size, num_classes)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# 超参数设置\n",
    "input_dim = 10  # 输入特征的维度（例如，患者的特征数量）\n",
    "model_dim = 64  # Transformer 模型的维度\n",
    "n_heads = 8     # 注意力头的数量\n",
    "num_classes = 2 # 分类数量（例如，黄牛与非黄牛）\n",
    "num_layers = 4  # Transformer 层数\n",
    "dropout = 0.1   # Dropout 比例\n",
    "\n",
    "# 创建模型\n",
    "model = TransLog(input_dim, model_dim, n_heads, num_classes, num_layers, dropout)\n",
    "\n",
    "# 示例输入\n",
    "seq_len = 5    # 序列长度\n",
    "batch_size = 3 # 批次大小\n",
    "example_input = torch.rand(seq_len, batch_size, input_dim)  # 随机生成示例输入\n",
    "\n",
    "# 前向传播\n",
    "output = model(example_input)\n",
    "\n",
    "print(\"模型输出形状:\", output.shape)  # 应该是 (batch_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 数据准备\n",
    "# # 假设您有以下数据\n",
    "# # features: (num_samples, seq_len, input_dim)\n",
    "# # labels: (num_samples)\n",
    "\n",
    "# # 生成示例数据\n",
    "# num_samples = 1000\n",
    "# seq_len = 5\n",
    "# input_dim = 10\n",
    "# num_classes = 2\n",
    "\n",
    "# features = torch.rand(num_samples, seq_len, input_dim)  # 随机生成特征\n",
    "# labels = torch.randint(0, num_classes, (num_samples,))   # 随机生成标签\n",
    "\n",
    "# # 切分数据集\n",
    "# X_train, X_val, y_train, y_val = train_test_split(features, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 模型训练\n",
    "# # 创建模型\n",
    "# model = TransLog(input_dim, model_dim, n_heads, num_classes, num_layers, dropout)\n",
    "\n",
    "# # 定义损失函数和优化器\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# # 训练参数\n",
    "# num_epochs = 20\n",
    "# batch_size = 32\n",
    "\n",
    "# # 训练循环\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()  # 设置模型为训练模式\n",
    "#     for i in range(0, len(X_train), batch_size):\n",
    "#         # 获取当前批次的输入和标签\n",
    "#         inputs = X_train[i:i + batch_size]\n",
    "#         labels = y_train[i:i + batch_size]\n",
    "\n",
    "#         optimizer.zero_grad()  # 清空梯度\n",
    "#         outputs = model(inputs)  # 前向传播\n",
    "#         loss = criterion(outputs, labels)  # 计算损失\n",
    "#         loss.backward()  # 反向传播\n",
    "#         optimizer.step()  # 更新参数\n",
    "\n",
    "#     print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 模型评估\n",
    "# model.eval()  # 设置模型为评估模式\n",
    "# with torch.no_grad():  # 禁用梯度计算\n",
    "#     val_outputs = model(X_val)\n",
    "#     _, predicted = torch.max(val_outputs, 1)  # 获取预测类别\n",
    "#     accuracy = (predicted == y_val).sum().item() / len(y_val) * 100  # 计算准确率\n",
    "\n",
    "# print(f'Validation Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 模型预测\n",
    "# def predict(model, input_sequence):\n",
    "#     model.eval()  # 设置模型为评估模式\n",
    "#     with torch.no_grad():  # 禁用梯度计算\n",
    "#         output = model(input_sequence)  # 前向传播\n",
    "#         _, predicted = torch.max(output, 1)  # 获取预测类别\n",
    "#     return predicted\n",
    "\n",
    "# # 示例输入\n",
    "# new_patient_sequence = torch.rand(1, seq_len, input_dim)  # 随机生成一个新患者的序列\n",
    "# prediction = predict(model, new_patient_sequence)\n",
    "\n",
    "# print(f'预测类别: {prediction.item()}')  # 输出预测结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 流放地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# # 特征选择\n",
    "# used_data = pd.DataFrame()\n",
    "# used_data['ID'] = data['ID']\n",
    "# used_data['start_t'] = data['订单创建时间'].dt.hour * 3600 + data['订单创建时间'].dt.minute * 60 + data['订单创建时间'].dt.second\n",
    "# used_data['delta_t'] = (data['就诊日期'] - data['订单创建时间']).astype('int64') / 10**9\n",
    "# used_data['pid'] = data['患者ID']\n",
    "\n",
    "# # 统计每个患者ID对应的不同APPID数量\n",
    "# app_num = data.groupby('患者ID')['APPID'].nunique().reset_index()\n",
    "# app_num.columns = ['pid', 'app_num']  # 重命名列\n",
    "# used_data = used_data.merge(app_num, on='pid', how='left')\n",
    "\n",
    "# # 每个患者的科室挂号数\n",
    "# dorm_num = data.groupby('患者ID')['就诊科室名称'].nunique().reset_index()\n",
    "# dorm_num.columns = ['pid', 'dorm_num']  # 重命名列\n",
    "# used_data = used_data.merge(dorm_num, on='pid', how='left')\n",
    "\n",
    "# used_data['province'] = data['省份']\n",
    "# used_data['ip'] = data['IP_ADDRESS'].apply(ip_encoder)\n",
    "# used_data['status'] = data['状态']\n",
    "\n",
    "# features = ['start_t', 'delta_t', 'pid', 'province', 'ip', 'status', 'app_num']\n",
    "\n",
    "# # 归一化\n",
    "# scaler = MinMaxScaler()  # 或使用 StandardScaler()\n",
    "# used_data[['start_t', 'delta_t', 'ip']] = scaler.fit_transform(used_data[['start_t', 'delta_t', 'ip']])\n",
    "\n",
    "# # 使用 ColumnTransformer 进行特征处理\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', SimpleImputer(strategy='constant'), ['delta_t', 'start_t', 'ip', 'app_num']),\n",
    "#         ('cat', OneHotEncoder(), ['pid', 'province', 'status'])  # 分类特征处理\n",
    "#     ],\n",
    "#     remainder='drop'\n",
    "# )\n",
    "# # 创建聚类管道\n",
    "# pipeline = Pipeline(steps=[\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('kmeans', KMeans(n_clusters=2, random_state=0))\n",
    "# ])\n",
    "\n",
    "# # 拟合模型\n",
    "# pipeline.fit(used_data[features])\n",
    "\n",
    "# # 获取聚类标签\n",
    "# used_data[f'cluster'] = pipeline.predict(used_data[features])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
