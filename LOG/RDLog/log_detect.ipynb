{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 黄牛检测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "file_path = './data/raw_data.xlsx'\n",
    "sheet_name = 'Sheet1'\n",
    "data = pd.read_excel(file_path, sheet_name=sheet_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 时间预处理\n",
    "data['订单创建时间'] = pd.to_datetime(data['订单创建时间'])\n",
    "data['就诊日期'] = pd.to_datetime(data['就诊日期'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ID', '省份', 'APPID', 'IP_ADDRESS', '订单创建时间', '患者ID', '患者创建时间', '就诊日期',\n",
      "       '就诊科室名称', '医生姓名', '状态', '商户订单号'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 写入到指定log文件中(LogBERT相关,已废弃)\n",
    "def write_log(log_file_path):\n",
    "    # 将数据逐行写入.log 文件\n",
    "    with open(log_file_path, 'w', encoding='utf-8') as log_file:\n",
    "        for index, row in data.iterrows():\n",
    "            # 将每一行转换为字符串并写入日志文件\n",
    "            row = row.to_dict()\n",
    "            log_file.write(row['患者ID'] + \" : \")\n",
    "            for k, v in row.items():\n",
    "                if k != \"患者ID\":\n",
    "                    log_file.write(str(v) + \" \")\n",
    "            log_file.write(\"\\n\")\n",
    "    print(f\"Data has been written to {log_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 基于规则的检测\n",
    "\n",
    "1. IP重复，来自同一个IP，且为超过3个人挂号\n",
    "2. 用户重复，来自同一个用户，且挂号了超过3个科室/超过两个app_id\n",
    "3. 时间过早，每天5:00-5:01进行操作的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重复值筛选，找出data的seg_name字段中重复数大于limit的行, sort_add和asc_add是检测完之后添加的排序要求\n",
    "def duplicate_detect(data, seg_name, limit, up=True, sort_add=[], asc_add=[]):\n",
    "    assert len(sort_add) == len(asc_add)\n",
    "    value_counts = data[seg_name].value_counts()\n",
    "    if up:\n",
    "        # 向上筛\n",
    "        dup_row = data[data[seg_name].isin(value_counts[value_counts > limit].index)]\n",
    "    else:\n",
    "        # 向下筛\n",
    "        dup_row = data[data[seg_name].isin(value_counts[value_counts < limit].index)]\n",
    "    sort_by = [seg_name] + sort_add\n",
    "    asc = [True] + asc_add\n",
    "    dup_row = dup_row.sort_values(by=sort_by, ascending=asc)\n",
    "    return dup_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 筛选出同一个dup_seg中有超过limit个unique_seg的ID字段\n",
    "def unique_dup_filter(data, dup_seg, unique_seg, limit, up=True, get_row=False):\n",
    "    # 确定是黄牛的行编号\n",
    "    selects = []\n",
    "    # IP重复检测\n",
    "    dup = duplicate_detect(data, dup_seg, limit, up=up, sort_add=[unique_seg], asc_add=[True])\n",
    "    print(\"init dup_num:\", len(dup))\n",
    "    if len(dup) == 0:\n",
    "        return selects\n",
    "    pos = 0          # 这个IP的起点\n",
    "    count = 1      # 涉及多少个用户ID\n",
    "    last = dup[unique_seg].iloc[0] # 上一个用户的ID\n",
    "    dup_num = len(dup)\n",
    "    for i in tqdm(range(1, dup_num)):\n",
    "        if dup[dup_seg].iloc[i] == dup[dup_seg].iloc[pos]:\n",
    "            if dup[unique_seg].iloc[i] != last:\n",
    "                # 相同IP下一个新的患者\n",
    "                count += 1\n",
    "                last = dup[unique_seg].iloc[i]\n",
    "        if dup[dup_seg].iloc[i] != dup[dup_seg].iloc[pos] or i == dup_num - 1:\n",
    "            # 开始检测下一个IP\n",
    "            if count > limit:\n",
    "                # 达到重复人数条件\n",
    "                for j in range(pos, i):\n",
    "                    selects.append(dup['ID'].iloc[j])\n",
    "            # 重置\n",
    "            pos = i\n",
    "            count = 1\n",
    "            last = dup[unique_seg].iloc[i]\n",
    "    print(\"filtered dup_num:\", len(selects))\n",
    "    if not get_row:\n",
    "        selects.sort()\n",
    "        return selects\n",
    "    else:\n",
    "        dup_rows = data[data['ID'].isin(selects)]\n",
    "        dup_rows = dup_rows.sort_values(by=dup_seg, ascending=True)\n",
    "        return dup_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对unique_dup_filter, 在一定范围内遍历limit(基本不用)\n",
    "# 使用: limits, select_list = grid_traverse(data, '患者ID', '就诊科室名称', 5, 10)\n",
    "def grid_traverse(data, dup_seg, unique_seg, start, end, gap=1):\n",
    "    limits = []\n",
    "    selects_list = []\n",
    "    for limit in range(start, end, gap):\n",
    "        print(f\"limit: {limit}\")\n",
    "        selects = unique_dup_filter(data, dup_seg, unique_seg, limit)\n",
    "        limits.append(limit)\n",
    "        selects_list.append(selects)\n",
    "    return limits, selects_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 时间段过滤器，过滤出每天一段时间内的数据\n",
    "# 形如daily_filter(data, '5:00:00', '5:01:00')\n",
    "def daily_filter(data, start, end):\n",
    "    return data[(data['订单创建时间'].dt.time >= pd.to_datetime(start).time()) &\n",
    "                    (data['订单创建时间'].dt.time <= pd.to_datetime(end).time())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 时间分段统计(数据分析而非处理, 只会打印出每个小时内各个操作的次数)\n",
    "def hour_count():\n",
    "    print(\"\\t\\t总数\\t已挂号  医保换号  已退号  窗口退号  无号退款  超时取消\")\n",
    "    for i in range(5, 23):\n",
    "        time_filter = data[(data['订单创建时间'].dt.time >= pd.to_datetime(f'{i}:00:00').time()) &\n",
    "                     (data['订单创建时间'].dt.time <= pd.to_datetime(f'{i+1}:00:00').time())]\n",
    "        counts = time_filter['状态'].value_counts()\n",
    "        print(f\"{i}:00 - {i+1}:00 \\t{len(time_filter)}\\t{counts['已挂号']}\\t{counts['医保换号']}\\t {counts['已退号']}\\t\"\n",
    "              f\"   {counts['窗口退号']}\\t   {counts['无号退款']}\\t    {counts['超时取消']}\\t\")\n",
    "\n",
    "# 一个小时内按分钟打印操作次数(数据分析而非处理)\n",
    "def minute_line(data, hour):\n",
    "    count = []\n",
    "    for i in tqdm(range(0, 59, 5)):\n",
    "        start = str(i)\n",
    "        end = str(i+5)\n",
    "        if len(start) < 2:\n",
    "            start = '0' + start\n",
    "        if len(end) < 2:\n",
    "            end = '0' + end \n",
    "        time_filter = data[(data['订单创建时间'].dt.time >= pd.to_datetime(f'{hour}:{start}:00').time()) &\n",
    "                     (data['订单创建时间'].dt.time <= pd.to_datetime(f'{hour}:{end}:00').time())]\n",
    "        count.append(len(time_filter))\n",
    "    plt.plot(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 恰好在16:00进行第二天/下一周操作(因为这里面很有可能有很多正常人,不建议使用)\n",
    "def hurry_sixteen(data, get_row=False):\n",
    "    gap = daily_filter(data, '16:00:00', '16:00:01')\n",
    "    time_diff = (gap['就诊日期'] - gap['订单创建时间']).dt.days\n",
    "    hurry_row = gap[(time_diff == 0) | (time_diff == 6)]\n",
    "    if get_row:\n",
    "        return hurry_row\n",
    "    else:\n",
    "        return hurry_row['ID'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将之前一次答案中的数据提取成列表(需要配套data文件夹中的result1/2/3...使用)\n",
    "def get_list(res_id, file_path=\"\"):\n",
    "    if file_path == \"\":\n",
    "        file_path = f'./data/result{res_id}.txt'\n",
    "    with open(file_path, 'r') as file:\n",
    "        lis = [int(line.strip()) for line in file]\n",
    "    return lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 两个list进行对比(相比增加的,减少的,相同的)\n",
    "def lis_cmp(lis1, lis2, ret=False, show=True):\n",
    "    set1 = set(lis1)\n",
    "    set2 = set(lis2)\n",
    "    new_ele = set2 - set1\n",
    "    miss_ele = set1 - set2\n",
    "    same_ele = set1.intersection(set2)\n",
    "    if show:\n",
    "        print(f\"lis1: {len(lis1)},\\tlis2: {len(lis2)},\\tmore: {len(new_ele)},\"\n",
    "            f\"\\tmiss: {len(miss_ele)},\\tsame = {len(same_ele)}\")\n",
    "    if ret:\n",
    "        return list(new_ele), list(miss_ele), list(same_ele)\n",
    "\n",
    "# 与之前一次答案进行对比\n",
    "def res_cmp(res_id, lis, ret=False, show=True):\n",
    "    lis1 = get_list(res_id)\n",
    "    res = lis_cmp(lis1, lis, ret, show)\n",
    "    if ret:\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将一个list写作答案(写到data/result.txt中,remove_white选项是是否去除白名单)\n",
    "def write_list(lis, remove_white=True):\n",
    "    if remove_white:\n",
    "        white_lis = get_list(0, './data/white.txt')\n",
    "        print(f\"remove white: {len(set(lis).intersection(set(white_lis)))}\")\n",
    "        lis = list(set(lis) - set(white_lis))\n",
    "        lis.sort()\n",
    "    # 打开一个文件进行写入，如果文件不存在则创建\n",
    "    with open('./data/result.txt', 'w', encoding='utf-8') as file:\n",
    "        # 遍历列表中的每个元素\n",
    "        for item in lis:\n",
    "            # 将每个元素写入文件，每个元素后面加上换行符\n",
    "            file.write(str(item) + '\\n')\n",
    "        print(f\"Total line: {len(lis)}\")\n",
    "    return lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data中退号超过limit词的(这条规则效果似乎不算太好)\n",
    "def frequent_drop(data, limit, get_row=False):\n",
    "    drop = data[data['状态'] == '已退号']\n",
    "    mass_drop = duplicate_detect(drop, \"患者ID\", limit)\n",
    "    if get_row:\n",
    "        return mass_drop['ID'].tolist()\n",
    "    else:\n",
    "        return mass_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简单筛选不是黄牛的用户(效果不确定)\n",
    "def normal_user(data):\n",
    "    # 黑名单\n",
    "    black_lis = get_list(6)\n",
    "    # 白名单\n",
    "    white_user1 = get_list(-1, file_path=\"./data/white.txt\")\n",
    "    black_rows = data[data['ID'].isin(black_lis)]\n",
    "    black_user = black_rows['患者ID'].to_list()\n",
    "    not_black = data[~data['患者ID'].isin(black_user)]['患者ID'].to_list()\n",
    "    white_sample = random.sample(not_black, 10 * len(white_user1))\n",
    "    white_user2 = data[data['患者ID'].isin(white_sample)]['ID'].to_list()\n",
    "    white_rows1 = data[data['ID'].isin(white_user1)]\n",
    "    white_rows2 = data[data['ID'].isin(white_user2)]\n",
    "    return white_rows1, white_rows2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据权重在多个不同的list中进行合并筛选\n",
    "def weighted_selection(lists, weights, n, limit=0):\n",
    "    # 创建一个字典来存储元素的总权重\n",
    "    total_weights = defaultdict(float)\n",
    "\n",
    "    # 遍历每个列表及其对应的权重\n",
    "    for lst, weight in zip(lists, weights):\n",
    "        for element in lst:\n",
    "            total_weights[element] += weight  # 累加权重\n",
    "\n",
    "    # 将字典转换为列表，并筛选出权重大于 limit 的元素\n",
    "    filtered_elements = {k: v for k, v in total_weights.items() if v >= limit}\n",
    "\n",
    "    # 按照权重排序\n",
    "    sorted_elements = sorted(filtered_elements.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # 选择前 n 个元素及其权重\n",
    "    top_n_elements = sorted_elements[:n]\n",
    "\n",
    "    return top_n_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正常人过滤器\n",
    "# 只要在时间范围内有一次(\"已挂号\")\n",
    "# 返回剩下的可疑数据\n",
    "def normal_filter(data, rows):\n",
    "    all_users = rows['患者ID'].to_list()\n",
    "    normal_users = data[(data['就诊日期'] <= pd.to_datetime('2024-4-9')) & (data['状态'] == \"已挂号\")]['患者ID'].to_list()\n",
    "    abnormal_users = list(set(all_users) - set(normal_users))\n",
    "    filtered_users = list(set(all_users).intersection(set(normal_users)))\n",
    "    abnormal_rows = data[data['患者ID'].isin(abnormal_users)]\n",
    "    normal_rows = data[data['患者ID'].isin(filtered_users)]\n",
    "    return abnormal_rows, normal_rows\n",
    "\n",
    "# 基础过滤器\n",
    "# 去除那些正常的就诊记录\n",
    "def basic_filter(data, select):\n",
    "    copy = list(select)\n",
    "    select_rows = data[data['ID'].isin(select)]\n",
    "    normal_ids = select_rows[(select_rows['就诊日期'] <= pd.to_datetime('2024-4-9')) & (select_rows['状态'] == \"已挂号\")]['ID'].to_list()\n",
    "    copy = list(set(copy) - set(normal_ids))\n",
    "    return copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 基于聚类的分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ip_encoder(ip):\n",
    "    parts = list(map(int, ip.split('.')))\n",
    "    return (parts[0] << 24) + (parts[1] << 16) + (parts[2] << 8) + parts[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 基于学习的分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "state2num = {\n",
    "    '已退号': 0,\n",
    "    '已挂号': 1,\n",
    "    '窗口退号': 2,\n",
    "    '医保换号': 3,\n",
    "    '无号退款': 4,\n",
    "    '超时取消': 5,\n",
    "    '主动取消': 6,\n",
    "    np.nan: 7\n",
    "}\n",
    "\n",
    "def get_embed_list(state_list_pd, seg_name, series_name):\n",
    "    state_lists = state_list_pd.groupby(seg_name)[series_name].apply(list).reset_index()\n",
    "    state_lists = state_lists[series_name].to_list()\n",
    "    for i in range(len(state_lists)):\n",
    "        state_lists[i] = [str(state2num[str_]) for str_ in state_lists[i]]\n",
    "    return state_lists\n",
    "\n",
    "def generate_train_test(data, seg_name, series_name, normal_func, limit=2000, path='../LogBERT/output/rd/'):\n",
    "    # train\n",
    "    normal_rows = normal_func(data)\n",
    "    normal_id = normal_rows[seg_name].to_list()\n",
    "    sample_id = random.sample(normal_id, limit)\n",
    "    normals = data[data[seg_name].isin(sample_id)]\n",
    "    state_lists = get_embed_list(normals, seg_name, series_name)\n",
    "    with open(path + \"train\", 'w') as train_file:\n",
    "        for states in state_lists:\n",
    "            train_file.write(\" \".join(states) + \"\\n\")\n",
    "\n",
    "    # test_normal\n",
    "    unsample_id = list(set(normal_id) - set(sample_id))\n",
    "    test_normals = data[data[seg_name].isin(unsample_id)]\n",
    "    state_lists = get_embed_list(test_normals, seg_name, series_name)\n",
    "    with open(path + \"test_normal\", 'w') as train_file:\n",
    "        for states in state_lists:\n",
    "            train_file.write(\" \".join(states) + \"\\n\")\n",
    "\n",
    "    # test_abnormal\n",
    "    res_id = get_list(5)\n",
    "    test_abnormals = data[data['ID'].isin(res_id)]\n",
    "    state_lists = get_embed_list(test_abnormals, seg_name, series_name)\n",
    "    with open(path + \"test_abnormal\", 'w') as train_file:\n",
    "        for states in state_lists:\n",
    "            train_file.write(\" \".join(states) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LogBERT 魔改记录\n",
    "\n",
    "1. logbert.py里的options[\"min_len\"]改成了1, epoch也改成了2\n",
    "2. sample.py中line = line.squeeze()改为line.ravel()\n",
    "3. sample.py中logkey_seq_pairs = np.array(logkey_seq_pairs,)加上了dtype=object，predict_log.py中如果报错也添加这个就行\n",
    "4. train_log.py中epoch > 0及生成center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 手写神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将一个list写入白名单\n",
    "def add_white(lis, path='./data/white.txt'):\n",
    "    former_list = []\n",
    "    if os.path.isfile(path):\n",
    "        former_list = get_list(0, path)\n",
    "    lis = list(set(lis + former_list))\n",
    "    with open(path, 'w', encoding='utf-8') as file:\n",
    "        for item in lis:\n",
    "            file.write(str(item) + '\\n')\n",
    "        print(f\"Total line: {len(lis)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 省份, 患者ID, 就诊科室名称, 医生姓名, 状态\n",
    "\n",
    "# 按照用户分组提取出操作的时间序列，然后嵌入成一个torch.tensor\n",
    "def feature_extract(rows):\n",
    "    res_list = []\n",
    "    user_ids = []\n",
    "    # 根据User分成dataFrame的list\n",
    "    user_group = rows.groupby('患者ID')\n",
    "    data_list = [group.reset_index(drop=True) for name, group in user_group]\n",
    "    \n",
    "    # 每一组根据时间进行排序\n",
    "    for i in range(len(data_list)):\n",
    "        data_list[i] = data_list[i].sort_values(by='就诊日期', ascending=True)\n",
    "        \n",
    "        prov_tensor = torch.tensor(data_list[i]['prov_id'].values).unsqueeze(0)\n",
    "        depart_tensor = torch.tensor(data_list[i]['depart_id'].values).unsqueeze(0)\n",
    "        doc_tensor = torch.tensor(data_list[i]['doc_id'].values).unsqueeze(0)\n",
    "        state_tensor = torch.tensor(data_list[i]['state_id'].values).unsqueeze(0)\n",
    "        embedding = torch.cat((prov_tensor, depart_tensor, doc_tensor, state_tensor), dim=0).T\n",
    "        res_list.append(embedding)\n",
    "        user_ids.append(data_list[i]['患者ID'][0])\n",
    "    \n",
    "    return res_list, user_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将tensor填充成指定行数\n",
    "def padding(tensor, target_length):\n",
    "    current_length = tensor.size(0)  # 当前行数\n",
    "    padding_amount = target_length - current_length\n",
    "    if padding_amount > 0:\n",
    "        padding_tensor = torch.zeros(padding_amount, tensor.size(1), dtype=tensor.dtype)\n",
    "        padded_tensor = torch.cat((tensor, padding_tensor), dim=0)\n",
    "    else:\n",
    "        # 如果当前行数已经大于或等于目标长度，直接使用原 tensor\n",
    "        padded_tensor = tensor[:target_length]\n",
    "    return padded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_user_statistic(data):\n",
    "    id_counts = data.groupby('患者ID').size()\n",
    "    for i in range(0, 20):\n",
    "        filtered_ids = id_counts[id_counts > i]\n",
    "        result_count = len(filtered_ids)\n",
    "        print(f'操作数大于 {i} 的 患者 个数: {result_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_loader(samples, labels, SequenceDataset, train_ratio):\n",
    "    samples_list = list(samples)\n",
    "    labels_list = list(labels)\n",
    "\n",
    "    # 设置划分比例\n",
    "    train_size = int(train_ratio * len(samples_list))\n",
    "    indices = np.random.permutation(len(samples_list))  # 随机打乱索引\n",
    "\n",
    "    # 划分索引\n",
    "    train_indices = indices[:train_size]\n",
    "    test_indices = indices[train_size:]\n",
    "\n",
    "    # 创建训练和测试数据集\n",
    "    train_samples = [samples_list[i] for i in train_indices]\n",
    "    train_labels = [labels_list[i] for i in train_indices]\n",
    "\n",
    "    test_samples = [samples_list[i] for i in test_indices]\n",
    "    test_labels = [labels_list[i] for i in test_indices]\n",
    "\n",
    "    train_dataset = SequenceDataset(train_samples, train_labels)\n",
    "    test_dataset = SequenceDataset(test_samples, test_labels)\n",
    "\n",
    "    # 创建 DataLoader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "# 先对类别列进行编码\n",
    "data = data.fillna(method='ffill')     # 前值替换缺失值\n",
    "data['prov_id'] = pd.factorize(data['省份'])[0]\n",
    "data['depart_id'] = pd.factorize(data['就诊科室名称'])[0]\n",
    "data['doc_id'] = pd.factorize(data['医生姓名'])[0]\n",
    "data['state_id'] = pd.factorize(data['状态'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备数据\n",
    "## 黄牛\n",
    "abnormal_id = get_list(6)\n",
    "abnormal_rows = data[data['ID'].isin(abnormal_id)]\n",
    "## 正常人\n",
    "normal_rows1, normal_rows2 = normal_user(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 黄牛特征向量\n",
    "abnormal_tensors, _ = feature_extract(abnormal_rows)\n",
    "# 普通人的特征向量\n",
    "normal_tensors1, _ = feature_extract(normal_rows1)\n",
    "normal_tensors2, _ = feature_extract(normal_rows2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_tensors1 = normal_tensors1 * 10\n",
    "normal_tensors = normal_tensors1 + normal_tensors2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建数据集\n",
    "## 标签数据\n",
    "normal_labels = torch.zeros(len(normal_tensors), dtype=torch.long)\n",
    "abnormal_labels = torch.ones(len(abnormal_tensors), dtype=torch.long)\n",
    "samples = normal_tensors + abnormal_tensors\n",
    "# 序列填充\n",
    "for i in range(len(samples)):\n",
    "    samples[i] = padding(samples[i], 15)\n",
    "labels = torch.cat((normal_labels, abnormal_labels))\n",
    "\n",
    "## 数据集\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, samples, labels):\n",
    "        self.samples = samples\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx], self.labels[idx]\n",
    "## 划分训练与测试\n",
    "train_loader, test_loader = train_test_loader(samples, labels, SequenceDataset, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型定义\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, model_dim, num_heads, num_classes, num_layers):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Linear(input_dim, model_dim)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(model_dim, num_heads),\n",
    "            num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(model_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_length, input_dim)\n",
    "        x = self.embedding(x)  # (batch_size, seq_length, model_dim)\n",
    "        x = x.transpose(0, 1)  # (seq_length, batch_size, model_dim) for Transformer\n",
    "        x = self.transformer_encoder(x)  # (seq_length, batch_size, model_dim)\n",
    "        x = x.mean(dim=0)  # Global average pooling\n",
    "        x = self.fc(x)  # (batch_size, num_classes)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Library\\Python\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "# 超参数设置\n",
    "# 超参数\n",
    "input_dim = 4  # 每个时间步的特征数量\n",
    "model_dim = 64  # Transformer 模型维度\n",
    "num_heads = 4  # 注意力头数量\n",
    "num_classes = 2  # 分类数量（正常和异常）\n",
    "num_layers = 2  # Transformer 层数\n",
    "\n",
    "# 创建模型，损失函数和优化器\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "model = TransformerModel(input_dim, model_dim, num_heads, num_classes, num_layers).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Library\\Python\\anaconda\\lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.1098\n",
      "Epoch [2/10], Loss: 0.3022\n",
      "Epoch [3/10], Loss: 0.4241\n",
      "Epoch [4/10], Loss: 0.0948\n",
      "Epoch [5/10], Loss: 0.1371\n",
      "Epoch [6/10], Loss: 0.1886\n",
      "Epoch [7/10], Loss: 0.1645\n",
      "Epoch [8/10], Loss: 0.3266\n",
      "Epoch [9/10], Loss: 0.0836\n",
      "Epoch [10/10], Loss: 0.1152\n"
     ]
    }
   ],
   "source": [
    "# 模型训练\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_samples, batch_labels in train_loader:\n",
    "        batch_samples = batch_samples.to(device).float()\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_samples)  # 前向传播\n",
    "        loss = criterion(outputs, batch_labels)  # 计算损失\n",
    "        loss.backward()  # 反向传播\n",
    "        optimizer.step()  # 优化器更新\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 94.52%\n"
     ]
    }
   ],
   "source": [
    "# 模型评估\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_samples, batch_labels in test_loader:\n",
    "        batch_samples = batch_samples.to(device).float()\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        outputs = model(batch_samples)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += batch_labels.size(0)\n",
    "        correct += (predicted == batch_labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实际使用\n",
    "model.eval()\n",
    "all_tensors, users = feature_extract(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225429/225429 [03:43<00:00, 1008.36it/s]\n"
     ]
    }
   ],
   "source": [
    "classify = []\n",
    "for i in tqdm(range(len(all_tensors))):\n",
    "    all_tensors[i] = padding(all_tensors[i], 15)\n",
    "    input = all_tensors[i].to(device).float().unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        output = model(input)\n",
    "    predicted_class = torch.argmax(output, dim=1)\n",
    "    classify.append(predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [i for i, value in enumerate(classify) if value == 1]\n",
    "abnormal_uids = [users[i] for i in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 工作区"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据分析结论\n",
    "\n",
    "1. 从第一次提交到第3次，涨幅均是来自于发现了科室限制规则中的其他数据，也就是说，其间新增的规则无效\n",
    "2. 基于用户-科室的筛选的作用已经充分利用，继续基于这条规则展开没有意义\n",
    "3. 一个黄牛用户可能并不总是黄牛，必须去掉答案中那些已正常就诊的记录"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "手动发现:\n",
    "1. T001074\n",
    "2. T001519\n",
    "3. T002529\n",
    "4. T056389\n",
    "5. T094377"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "规划\n",
    "1. 之前的有没有被改动过 - 没改过\n",
    "2. 关键测试——是否再基于这条规则做以用户为分类的检测\n",
    "3. 检查过滤器是否起效"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 暂存区"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 多次退号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225429/225429 [05:06<00:00, 734.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "drop_users = []\n",
    "for name, group in tqdm(users):\n",
    "    user = group.reset_index()\n",
    "    drop_lines = user[user['状态'] == '已退号']\n",
    "    get_lines = user[(user['就诊日期'] <= pd.to_datetime('2024-4-9')) & (user['状态'] != \"已退号\")]\n",
    "    if len(drop_lines) > 3 and len(get_lines) < 2:\n",
    "        drop_users.append(user)\n",
    "print(len(drop_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_names = []\n",
    "for user in drop_users:\n",
    "    user_names.append(user['患者ID'][0])\n",
    "drop_rows = data[(data['患者ID'].isin(user_names)) & (data['状态'] == \"已退号\")]\n",
    "drop_rows = drop_rows.sort_values(by=['患者ID', '订单创建时间'], ascending=[True, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init dup_num: 105623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105622/105622 [00:02<00:00, 35439.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered dup_num: 4328\n",
      "Total line: 9579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[65537,\n",
       " 98310,\n",
       " 196615,\n",
       " 327689,\n",
       " 327691,\n",
       " 32780,\n",
       " 196622,\n",
       " 262161,\n",
       " 196628,\n",
       " 65558,\n",
       " 196629,\n",
       " 196636,\n",
       " 98337,\n",
       " 98345,\n",
       " 65582,\n",
       " 131118,\n",
       " 229423,\n",
       " 262195,\n",
       " 131130,\n",
       " 98365,\n",
       " 32834,\n",
       " 69,\n",
       " 32840,\n",
       " 163913,\n",
       " 32846,\n",
       " 196691,\n",
       " 65631,\n",
       " 131171,\n",
       " 32872,\n",
       " 65641,\n",
       " 229484,\n",
       " 32879,\n",
       " 163952,\n",
       " 196731,\n",
       " 32891,\n",
       " 229501,\n",
       " 65665,\n",
       " 98435,\n",
       " 327816,\n",
       " 131213,\n",
       " 98455,\n",
       " 131224,\n",
       " 229528,\n",
       " 131227,\n",
       " 262303,\n",
       " 131238,\n",
       " 164006,\n",
       " 98473,\n",
       " 295082,\n",
       " 262313,\n",
       " 32943,\n",
       " 164015,\n",
       " 177,\n",
       " 32945,\n",
       " 229555,\n",
       " 295095,\n",
       " 327872,\n",
       " 196803,\n",
       " 98499,\n",
       " 295112,\n",
       " 65740,\n",
       " 196819,\n",
       " 262356,\n",
       " 295127,\n",
       " 33004,\n",
       " 164077,\n",
       " 164080,\n",
       " 246,\n",
       " 65786,\n",
       " 131327,\n",
       " 65799,\n",
       " 196875,\n",
       " 33043,\n",
       " 327956,\n",
       " 33046,\n",
       " 283,\n",
       " 196892,\n",
       " 65818,\n",
       " 229661,\n",
       " 262427,\n",
       " 98595,\n",
       " 295208,\n",
       " 33066,\n",
       " 131374,\n",
       " 303,\n",
       " 98620,\n",
       " 262462,\n",
       " 196931,\n",
       " 295235,\n",
       " 131398,\n",
       " 98634,\n",
       " 65869,\n",
       " 337,\n",
       " 98644,\n",
       " 196949,\n",
       " 196951,\n",
       " 345,\n",
       " 98657,\n",
       " 196965,\n",
       " 98665,\n",
       " 328048,\n",
       " 328059,\n",
       " 131452,\n",
       " 164220,\n",
       " 33150,\n",
       " 196998,\n",
       " 295302,\n",
       " 65930,\n",
       " 328076,\n",
       " 262546,\n",
       " 98707,\n",
       " 98712,\n",
       " 409,\n",
       " 262557,\n",
       " 131488,\n",
       " 229792,\n",
       " 98724,\n",
       " 131493,\n",
       " 328100,\n",
       " 295336,\n",
       " 33200,\n",
       " 131504,\n",
       " 98751,\n",
       " 33216,\n",
       " 197065,\n",
       " 164300,\n",
       " 98764,\n",
       " 229841,\n",
       " 131539,\n",
       " 229850,\n",
       " 131550,\n",
       " 197088,\n",
       " 131556,\n",
       " 197105,\n",
       " 262643,\n",
       " 262646,\n",
       " 98809,\n",
       " 515,\n",
       " 98821,\n",
       " 164358,\n",
       " 131595,\n",
       " 131596,\n",
       " 328205,\n",
       " 229909,\n",
       " 197141,\n",
       " 295445,\n",
       " 295452,\n",
       " 262691,\n",
       " 66085,\n",
       " 131624,\n",
       " 131633,\n",
       " 66101,\n",
       " 197177,\n",
       " 131643,\n",
       " 131647,\n",
       " 98880,\n",
       " 131649,\n",
       " 328258,\n",
       " 66118,\n",
       " 262731,\n",
       " 295502,\n",
       " 328272,\n",
       " 197201,\n",
       " 262737,\n",
       " 328273,\n",
       " 66134,\n",
       " 33367,\n",
       " 197208,\n",
       " 66136,\n",
       " 262746,\n",
       " 66141,\n",
       " 229983,\n",
       " 613,\n",
       " 615,\n",
       " 619,\n",
       " 229998,\n",
       " 628,\n",
       " 328310,\n",
       " 230007,\n",
       " 131708,\n",
       " 262780,\n",
       " 164478,\n",
       " 131714,\n",
       " 131715,\n",
       " 262786,\n",
       " 66183,\n",
       " 66184,\n",
       " 328328,\n",
       " 197260,\n",
       " 328346,\n",
       " 328350,\n",
       " 98978,\n",
       " 197282,\n",
       " 197284,\n",
       " 66212,\n",
       " 164537,\n",
       " 197314,\n",
       " 262852,\n",
       " 230086,\n",
       " 328390,\n",
       " 230088,\n",
       " 197321,\n",
       " 230091,\n",
       " 99024,\n",
       " 164561,\n",
       " 722,\n",
       " 230098,\n",
       " 66283,\n",
       " 328432,\n",
       " 131825,\n",
       " 230136,\n",
       " 197373,\n",
       " 768,\n",
       " 131841,\n",
       " 295691,\n",
       " 66320,\n",
       " 328480,\n",
       " 131873,\n",
       " 131883,\n",
       " 131887,\n",
       " 131889,\n",
       " 164658,\n",
       " 262967,\n",
       " 328509,\n",
       " 33598,\n",
       " 131904,\n",
       " 262979,\n",
       " 328515,\n",
       " 131907,\n",
       " 197447,\n",
       " 164685,\n",
       " 197453,\n",
       " 328530,\n",
       " 328538,\n",
       " 864,\n",
       " 99174,\n",
       " 230247,\n",
       " 263015,\n",
       " 263027,\n",
       " 328578,\n",
       " 899,\n",
       " 263045,\n",
       " 131984,\n",
       " 197526,\n",
       " 328598,\n",
       " 263067,\n",
       " 33697,\n",
       " 99234,\n",
       " 99235,\n",
       " 328618,\n",
       " 132017,\n",
       " 263089,\n",
       " 132019,\n",
       " 230326,\n",
       " 33721,\n",
       " 66495,\n",
       " 132036,\n",
       " 99276,\n",
       " 263117,\n",
       " 132046,\n",
       " 132052,\n",
       " 263129,\n",
       " 197599,\n",
       " 99301,\n",
       " 999,\n",
       " 230398,\n",
       " 328702,\n",
       " 164871,\n",
       " 263176,\n",
       " 66574,\n",
       " 164897,\n",
       " 33830,\n",
       " 132135,\n",
       " 197673,\n",
       " 1065,\n",
       " 132137,\n",
       " 99372,\n",
       " 132147,\n",
       " 263220,\n",
       " 328758,\n",
       " 295992,\n",
       " 263227,\n",
       " 132160,\n",
       " 33858,\n",
       " 33860,\n",
       " 296007,\n",
       " 263240,\n",
       " 263242,\n",
       " 263249,\n",
       " 296024,\n",
       " 263256,\n",
       " 1117,\n",
       " 99427,\n",
       " 197731,\n",
       " 328805,\n",
       " 33894,\n",
       " 197733,\n",
       " 66670,\n",
       " 263283,\n",
       " 33907,\n",
       " 328820,\n",
       " 99450,\n",
       " 263296,\n",
       " 263300,\n",
       " 328852,\n",
       " 263318,\n",
       " 1176,\n",
       " 230552,\n",
       " 263326,\n",
       " 230561,\n",
       " 263333,\n",
       " 230566,\n",
       " 197798,\n",
       " 263337,\n",
       " 33964,\n",
       " 296122,\n",
       " 328893,\n",
       " 99518,\n",
       " 263362,\n",
       " 296136,\n",
       " 33994,\n",
       " 99532,\n",
       " 263374,\n",
       " 34006,\n",
       " 263385,\n",
       " 132316,\n",
       " 328929,\n",
       " 263397,\n",
       " 230630,\n",
       " 34024,\n",
       " 230631,\n",
       " 99565,\n",
       " 328943,\n",
       " 263408,\n",
       " 296177,\n",
       " 1277,\n",
       " 296190,\n",
       " 132350,\n",
       " 165120,\n",
       " 99584,\n",
       " 132357,\n",
       " 1286,\n",
       " 1290,\n",
       " 1305,\n",
       " 34082,\n",
       " 66851,\n",
       " 99618,\n",
       " 165154,\n",
       " 263463,\n",
       " 99625,\n",
       " 230697,\n",
       " 99628,\n",
       " 263475,\n",
       " 99637,\n",
       " 132405,\n",
       " 197944,\n",
       " 34106,\n",
       " 329019,\n",
       " 329020,\n",
       " 165190,\n",
       " 99659,\n",
       " 34127,\n",
       " 132432,\n",
       " 34129,\n",
       " 197970,\n",
       " 1371,\n",
       " 230749,\n",
       " 263517,\n",
       " 34147,\n",
       " 296293,\n",
       " 329062,\n",
       " 1386,\n",
       " 263531,\n",
       " 296300,\n",
       " 329068,\n",
       " 1409,\n",
       " 329092,\n",
       " 230793,\n",
       " 34192,\n",
       " 329111,\n",
       " 198046,\n",
       " 66982,\n",
       " 165290,\n",
       " 329137,\n",
       " 296371,\n",
       " 132542,\n",
       " 230849,\n",
       " 230851,\n",
       " 263619,\n",
       " 99786,\n",
       " 230859,\n",
       " 230863,\n",
       " 132561,\n",
       " 296411,\n",
       " 263655,\n",
       " 1512,\n",
       " 1516,\n",
       " 132591,\n",
       " 230898,\n",
       " 34292,\n",
       " 34293,\n",
       " 1526,\n",
       " 329205,\n",
       " 230918,\n",
       " 99846,\n",
       " 329222,\n",
       " 34326,\n",
       " 67094,\n",
       " 1564,\n",
       " 34333,\n",
       " 329246,\n",
       " 165412,\n",
       " 230948,\n",
       " 198182,\n",
       " 132653,\n",
       " 230967,\n",
       " 329284,\n",
       " 198215,\n",
       " 329294,\n",
       " 1617,\n",
       " 165463,\n",
       " 34392,\n",
       " 99929,\n",
       " 296536,\n",
       " 329304,\n",
       " 329319,\n",
       " 231021,\n",
       " 1651,\n",
       " 1658,\n",
       " 296574,\n",
       " 263814,\n",
       " 132743,\n",
       " 165512,\n",
       " 231050,\n",
       " 132747,\n",
       " 329354,\n",
       " 263823,\n",
       " 263825,\n",
       " 34452,\n",
       " 329375,\n",
       " 132769,\n",
       " 165538,\n",
       " 231074,\n",
       " 34468,\n",
       " 231078,\n",
       " 198310,\n",
       " 296614,\n",
       " 132781,\n",
       " 231085,\n",
       " 329389,\n",
       " 329394,\n",
       " 165558,\n",
       " 132790,\n",
       " 67256,\n",
       " 263865,\n",
       " 165569,\n",
       " 263878,\n",
       " 100040,\n",
       " 100044,\n",
       " 263890,\n",
       " 263891,\n",
       " 263893,\n",
       " 34522,\n",
       " 263914,\n",
       " 165612,\n",
       " 231155,\n",
       " 165620,\n",
       " 34554,\n",
       " 1787,\n",
       " 100092,\n",
       " 1798,\n",
       " 231175,\n",
       " 231179,\n",
       " 1807,\n",
       " 132879,\n",
       " 263957,\n",
       " 329496,\n",
       " 1823,\n",
       " 34594,\n",
       " 329508,\n",
       " 231214,\n",
       " 231218,\n",
       " 231224,\n",
       " 132921,\n",
       " 132923,\n",
       " 296761,\n",
       " 231230,\n",
       " 34626,\n",
       " 1860,\n",
       " 165700,\n",
       " 34637,\n",
       " 198484,\n",
       " 264020,\n",
       " 329558,\n",
       " 100202,\n",
       " 132973,\n",
       " 67439,\n",
       " 264047,\n",
       " 67449,\n",
       " 329594,\n",
       " 165759,\n",
       " 296830,\n",
       " 329601,\n",
       " 231302,\n",
       " 133004,\n",
       " 165772,\n",
       " 1937,\n",
       " 34707,\n",
       " 198551,\n",
       " 231328,\n",
       " 1955,\n",
       " 133027,\n",
       " 264109,\n",
       " 296879,\n",
       " 198580,\n",
       " 165815,\n",
       " 100287,\n",
       " 264147,\n",
       " 34772,\n",
       " 264160,\n",
       " 198627,\n",
       " 2020,\n",
       " 231398,\n",
       " 133096,\n",
       " 100329,\n",
       " 231400,\n",
       " 329704,\n",
       " 264173,\n",
       " 264188,\n",
       " 165887,\n",
       " 329730,\n",
       " 296968,\n",
       " 165897,\n",
       " 198667,\n",
       " 165901,\n",
       " 165903,\n",
       " 296980,\n",
       " 264214,\n",
       " 264219,\n",
       " 264221,\n",
       " 231459,\n",
       " 2092,\n",
       " 67634,\n",
       " 297014,\n",
       " 231479,\n",
       " 133179,\n",
       " 198717,\n",
       " 329806,\n",
       " 2126,\n",
       " 67665,\n",
       " 34899,\n",
       " 329812,\n",
       " 2133,\n",
       " 133209,\n",
       " 264282,\n",
       " 34914,\n",
       " 165987,\n",
       " 67685,\n",
       " 329859,\n",
       " 264328,\n",
       " 297097,\n",
       " 67728,\n",
       " 198801,\n",
       " 231573,\n",
       " 231578,\n",
       " 231580,\n",
       " 34977,\n",
       " 329893,\n",
       " 133288,\n",
       " 67758,\n",
       " 231603,\n",
       " 100532,\n",
       " 231605,\n",
       " 133309,\n",
       " 35007,\n",
       " 35008,\n",
       " 35009,\n",
       " 166083,\n",
       " 35012,\n",
       " 35013,\n",
       " 264389,\n",
       " 35020,\n",
       " 100559,\n",
       " 133326,\n",
       " 297169,\n",
       " 198872,\n",
       " 264418,\n",
       " 2274,\n",
       " 166120,\n",
       " 35050,\n",
       " 329962,\n",
       " 166124,\n",
       " 329967,\n",
       " 2286,\n",
       " 35057,\n",
       " 67847,\n",
       " 35083,\n",
       " 166157,\n",
       " 231702,\n",
       " 35095,\n",
       " 330009,\n",
       " 166170,\n",
       " 198940,\n",
       " 100640,\n",
       " 231713,\n",
       " 67885,\n",
       " 100660,\n",
       " 100661,\n",
       " 297268,\n",
       " 297269,\n",
       " 198970,\n",
       " 35131,\n",
       " 330045,\n",
       " 100679,\n",
       " 330065,\n",
       " 2393,\n",
       " 166237,\n",
       " 166240,\n",
       " 2404,\n",
       " 67946,\n",
       " 231791,\n",
       " 35186,\n",
       " 35198,\n",
       " 133504,\n",
       " 100737,\n",
       " 231808,\n",
       " 264596,\n",
       " 2453,\n",
       " 100758,\n",
       " 264600,\n",
       " 166293,\n",
       " 2459,\n",
       " 199069,\n",
       " 133536,\n",
       " 231840,\n",
       " 297383,\n",
       " 330167,\n",
       " 2488,\n",
       " 100794,\n",
       " 2492,\n",
       " 100799,\n",
       " 166336,\n",
       " 199104,\n",
       " 231870,\n",
       " 264645,\n",
       " 330183,\n",
       " 264651,\n",
       " 330192,\n",
       " 133587,\n",
       " 35284,\n",
       " 297428,\n",
       " 68056,\n",
       " 297441,\n",
       " 100837,\n",
       " 231925,\n",
       " 297463,\n",
       " 2554,\n",
       " 100859,\n",
       " 133629,\n",
       " 68099,\n",
       " 166407,\n",
       " 297495,\n",
       " 264728,\n",
       " 68127,\n",
       " 330275,\n",
       " 231972,\n",
       " 330278,\n",
       " 330282,\n",
       " 166446,\n",
       " 133679,\n",
       " 68147,\n",
       " 166459,\n",
       " 2621,\n",
       " 330313,\n",
       " 2634,\n",
       " 35401,\n",
       " 35405,\n",
       " 2647,\n",
       " 166487,\n",
       " 2659,\n",
       " 133736,\n",
       " 166505,\n",
       " 133738,\n",
       " 133741,\n",
       " 232059,\n",
       " 2688,\n",
       " 232074,\n",
       " 199311,\n",
       " 35480,\n",
       " 199324,\n",
       " 35492,\n",
       " 232102,\n",
       " 264874,\n",
       " 133807,\n",
       " 133811,\n",
       " 264889,\n",
       " 232124,\n",
       " 199361,\n",
       " 133827,\n",
       " 68294,\n",
       " 35532,\n",
       " 2767,\n",
       " 232144,\n",
       " 232149,\n",
       " 2774,\n",
       " 101079,\n",
       " 35550,\n",
       " 101086,\n",
       " 133856,\n",
       " 101095,\n",
       " 68329,\n",
       " 199406,\n",
       " 35577,\n",
       " 68345,\n",
       " 2811,\n",
       " 68350,\n",
       " 35584,\n",
       " 68352,\n",
       " 35588,\n",
       " 101124,\n",
       " 166661,\n",
       " 264966,\n",
       " 330506,\n",
       " 101133,\n",
       " 133906,\n",
       " 68373,\n",
       " 2838,\n",
       " 101142,\n",
       " 133909,\n",
       " 297755,\n",
       " 199459,\n",
       " 330546,\n",
       " 265011,\n",
       " 265016,\n",
       " 330553,\n",
       " 232257,\n",
       " 199489,\n",
       " 68423,\n",
       " 101191,\n",
       " 2890,\n",
       " 2893,\n",
       " 297810,\n",
       " 330579,\n",
       " 133974,\n",
       " 133977,\n",
       " 2906,\n",
       " 232287,\n",
       " 232290,\n",
       " 265059,\n",
       " 68462,\n",
       " 232307,\n",
       " 166771,\n",
       " 101240,\n",
       " 199555,\n",
       " 2950,\n",
       " 68488,\n",
       " 68491,\n",
       " 166796,\n",
       " 2960,\n",
       " 35730,\n",
       " 134031,\n",
       " 35732,\n",
       " 232343,\n",
       " 101279,\n",
       " 134065,\n",
       " 297925,\n",
       " 101319,\n",
       " 35785,\n",
       " 101325,\n",
       " 68561,\n",
       " 199636,\n",
       " 232405,\n",
       " 199642,\n",
       " 101340,\n",
       " 297952,\n",
       " 101348,\n",
       " 101351,\n",
       " 199659,\n",
       " 134127,\n",
       " 232437,\n",
       " 101367,\n",
       " 101370,\n",
       " 199682,\n",
       " 68622,\n",
       " 199698,\n",
       " 101398,\n",
       " 101399,\n",
       " 232472,\n",
       " 330782,\n",
       " 3107,\n",
       " 166950,\n",
       " 199723,\n",
       " 35887,\n",
       " 298039,\n",
       " 298043,\n",
       " 3136,\n",
       " 166976,\n",
       " 232514,\n",
       " 298051,\n",
       " 298058,\n",
       " 101452,\n",
       " 298062,\n",
       " 68686,\n",
       " 3152,\n",
       " 35922,\n",
       " 35927,\n",
       " 134239,\n",
       " 35936,\n",
       " 265312,\n",
       " 35942,\n",
       " 232551,\n",
       " 298093,\n",
       " 3190,\n",
       " 298102,\n",
       " 167033,\n",
       " 134267,\n",
       " 134274,\n",
       " 134275,\n",
       " 167042,\n",
       " 35975,\n",
       " 35981,\n",
       " 265361,\n",
       " 134290,\n",
       " 167058,\n",
       " 101528,\n",
       " 265368,\n",
       " 298137,\n",
       " 101534,\n",
       " 232607,\n",
       " 199842,\n",
       " 298149,\n",
       " 265384,\n",
       " 330922,\n",
       " 134316,\n",
       " 298162,\n",
       " 232635,\n",
       " 199872,\n",
       " 330944,\n",
       " 3266,\n",
       " 232645,\n",
       " 101581,\n",
       " 101582,\n",
       " 167117,\n",
       " 265422,\n",
       " 68820,\n",
       " 199895,\n",
       " 167127,\n",
       " 134368,\n",
       " 167145,\n",
       " 36090,\n",
       " 36092,\n",
       " 167178,\n",
       " 232715,\n",
       " 167181,\n",
       " 36109,\n",
       " 36115,\n",
       " 68885,\n",
       " 36117,\n",
       " 199959,\n",
       " 232727,\n",
       " 167193,\n",
       " 199966,\n",
       " 101663,\n",
       " 3364,\n",
       " 199973,\n",
       " 167206,\n",
       " 167207,\n",
       " 232746,\n",
       " 199980,\n",
       " 199984,\n",
       " 298290,\n",
       " 265527,\n",
       " 134456,\n",
       " 232765,\n",
       " 101696,\n",
       " 298307,\n",
       " 167235,\n",
       " 167238,\n",
       " 331091,\n",
       " 298325,\n",
       " 298329,\n",
       " 167259,\n",
       " 36189,\n",
       " 68962,\n",
       " 3430,\n",
       " 101738,\n",
       " 167275,\n",
       " 265581,\n",
       " 200049,\n",
       " 3442,\n",
       " 134515,\n",
       " 68979,\n",
       " 3447,\n",
       " 232824,\n",
       " 101755,\n",
       " 3454,\n",
       " 331138,\n",
       " 167300,\n",
       " 200069,\n",
       " 298373,\n",
       " 101769,\n",
       " 36234,\n",
       " 167307,\n",
       " 69011,\n",
       " 3487,\n",
       " 167327,\n",
       " 167340,\n",
       " 331180,\n",
       " 36272,\n",
       " 200113,\n",
       " 167345,\n",
       " 200118,\n",
       " 134583,\n",
       " 3519,\n",
       " 101825,\n",
       " 265679,\n",
       " 232915,\n",
       " 3544,\n",
       " 101849,\n",
       " 167387,\n",
       " 101852,\n",
       " 3549,\n",
       " 69092,\n",
       " 69095,\n",
       " 101863,\n",
       " 101870,\n",
       " 167406,\n",
       " 331253,\n",
       " 200183,\n",
       " 167425,\n",
       " 134663,\n",
       " 69132,\n",
       " 298515,\n",
       " 298521,\n",
       " 298522,\n",
       " 134684,\n",
       " 331296,\n",
       " 232993,\n",
       " 200227,\n",
       " 36392,\n",
       " 3627,\n",
       " 200238,\n",
       " 101941,\n",
       " 298551,\n",
       " 3640,\n",
       " 265784,\n",
       " 200256,\n",
       " 331335,\n",
       " 298569,\n",
       " 36427,\n",
       " 298572,\n",
       " 298576,\n",
       " 233048,\n",
       " 167518,\n",
       " 134761,\n",
       " 3690,\n",
       " 36459,\n",
       " 167529,\n",
       " 167530,\n",
       " 233073,\n",
       " 134788,\n",
       " 331399,\n",
       " 298632,\n",
       " 200337,\n",
       " 298641,\n",
       " 233109,\n",
       " 102038,\n",
       " 200344,\n",
       " 298649,\n",
       " 233114,\n",
       " 134813,\n",
       " 167583,\n",
       " 69280,\n",
       " 265903,\n",
       " 298674,\n",
       " 298675,\n",
       " 3764,\n",
       " 298682,\n",
       " 233149,\n",
       " 69311,\n",
       " 331462,\n",
       " 298699,\n",
       " 233170,\n",
       " 265942,\n",
       " 102103,\n",
       " 200418,\n",
       " 3810,\n",
       " 233188,\n",
       " 3812,\n",
       " 134885,\n",
       " 331498,\n",
       " 298740,\n",
       " 331514,\n",
       " 233216,\n",
       " 331525,\n",
       " 69384,\n",
       " 102161,\n",
       " 3859,\n",
       " 134940,\n",
       " 3869,\n",
       " ...]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第十五次测试(11.21)\n",
    "# 基于猜测可能有效的规则\n",
    "# 1. 急速抢票\n",
    "select1 = daily_filter(data, '5:00:00', '5:00:01')['ID'].tolist()\n",
    "# 2. 同一用户使用超过2个APPID操作\n",
    "select2 = unique_dup_filter(data, '患者ID', 'APPID', 2)\n",
    "# 3. 第六次的记录\n",
    "select3 = get_list(6)\n",
    "select = list(set(select1 + select2 + select3))\n",
    "# 滤去正常挂号的\n",
    "select = basic_filter(data, select)\n",
    "write_list(select, remove_white=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
