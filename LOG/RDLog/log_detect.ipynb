{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 黄牛检测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import csv\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "file_path = './data/raw_data.xlsx'\n",
    "sheet_name = 'Sheet1'\n",
    "data = pd.read_excel(file_path, sheet_name=sheet_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 时间预处理\n",
    "data['订单创建时间'] = pd.to_datetime(data['订单创建时间'])\n",
    "data['就诊日期'] = pd.to_datetime(data['就诊日期'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 写入到指定log文件中\n",
    "def write_log(log_file_path):\n",
    "    # 将数据逐行写入.log 文件\n",
    "    with open(log_file_path, 'w', encoding='utf-8') as log_file:\n",
    "        for index, row in data.iterrows():\n",
    "            # 将每一行转换为字符串并写入日志文件\n",
    "            row = row.to_dict()\n",
    "            log_file.write(row['患者ID'] + \" : \")\n",
    "            for k, v in row.items():\n",
    "                if k != \"患者ID\":\n",
    "                    log_file.write(str(v) + \" \")\n",
    "            log_file.write(\"\\n\")\n",
    "    print(f\"Data has been written to {log_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 基于规则的检测\n",
    "\n",
    "1. IP重复，来自同一个IP，且为超过3个人挂号\n",
    "2. 用户重复，来自同一个用户，且挂号了超过3个科室/超过两个app_id\n",
    "3. 时间过早，每天5:00-5:01进行操作的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重复值筛选，找出data的seg_name字段中重复数大于limit的行, sort_add和asc_add是检测完之后添加的排序要求\n",
    "def duplicate_detect(data, seg_name, limit, up=True, sort_add=[], asc_add=[]):\n",
    "    assert len(sort_add) == len(asc_add)\n",
    "    value_counts = data[seg_name].value_counts()\n",
    "    if up:\n",
    "        # 向上筛\n",
    "        dup_row = data[data[seg_name].isin(value_counts[value_counts > limit].index)]\n",
    "    else:\n",
    "        # 向下筛\n",
    "        dup_row = data[data[seg_name].isin(value_counts[value_counts < limit].index)]\n",
    "    sort_by = [seg_name] + sort_add\n",
    "    asc = [True] + asc_add\n",
    "    dup_row = dup_row.sort_values(by=sort_by, ascending=asc)\n",
    "    return dup_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 筛选出同一个dup_seg中有超过limit个unique_seg的索引(从0开始)\n",
    "def unique_dup_filter(data, dup_seg, unique_seg, limit, up=True, get_row=False):\n",
    "    # 确定是黄牛的行编号\n",
    "    selects = []\n",
    "    # IP重复检测\n",
    "    dup = duplicate_detect(data, dup_seg, limit, up=up, sort_add=[unique_seg], asc_add=[True])\n",
    "    print(\"init dup_num:\", len(dup))\n",
    "    if len(dup) == 0:\n",
    "        return selects\n",
    "    pos = 0          # 这个IP的起点\n",
    "    count = 1      # 涉及多少个用户ID\n",
    "    last = dup[unique_seg].iloc[0] # 上一个用户的ID\n",
    "    dup_num = len(dup)\n",
    "    for i in tqdm(range(1, dup_num)):\n",
    "        if dup[dup_seg].iloc[i] == dup[dup_seg].iloc[pos]:\n",
    "            if dup[unique_seg].iloc[i] != last:\n",
    "                # 相同IP下一个新的患者\n",
    "                count += 1\n",
    "                last = dup[unique_seg].iloc[i]\n",
    "        if dup[dup_seg].iloc[i] != dup[dup_seg].iloc[pos] or i == dup_num - 1:\n",
    "            # 开始检测下一个IP\n",
    "            if count > limit:\n",
    "                # 达到重复人数条件\n",
    "                for j in range(pos, i):\n",
    "                    selects.append(dup['ID'].iloc[j])\n",
    "            # 重置\n",
    "            pos = i\n",
    "            count = 1\n",
    "            last = dup[unique_seg].iloc[i]\n",
    "    print(\"filtered dup_num:\", len(selects))\n",
    "    if not get_row:\n",
    "        selects.sort()\n",
    "        return selects\n",
    "    else:\n",
    "        dup_rows = data[data['ID'].isin(selects)]\n",
    "        dup_rows = dup_rows.sort_values(by=dup_seg, ascending=True)\n",
    "        return dup_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对unique_dup_filter, 在一定范围内遍历limit\n",
    "# 使用: limits, select_list = grid_traverse(data, '患者ID', '就诊科室名称', 5, 10)\n",
    "def grid_traverse(data, dup_seg, unique_seg, start, end, gap=1):\n",
    "    limits = []\n",
    "    selects_list = []\n",
    "    for limit in range(start, end, gap):\n",
    "        print(f\"limit: {limit}\")\n",
    "        selects = unique_dup_filter(data, dup_seg, unique_seg, limit)\n",
    "        limits.append(limit)\n",
    "        selects_list.append(selects)\n",
    "    return limits, selects_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将一个list写作答案\n",
    "def write_list(lis):\n",
    "    # 打开一个文件进行写入，如果文件不存在则创建\n",
    "    with open('./data/result.txt', 'w', encoding='utf-8') as file:\n",
    "        # 遍历列表中的每个元素\n",
    "        for item in lis:\n",
    "            # 将每个元素写入文件，每个元素后面加上换行符\n",
    "            file.write(str(item) + '\\n')\n",
    "        print(f\"Total line: {len(lis)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 时间段过滤器，过滤出每天一段时间内的数据\n",
    "# 形如daily_filter(data, '5:00:00', '5:01:00')\n",
    "def daily_filter(data, start, end):\n",
    "    return data[(data['订单创建时间'].dt.time >= pd.to_datetime(start).time()) &\n",
    "                    (data['订单创建时间'].dt.time <= pd.to_datetime(end).time())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 时间分段统计\n",
    "def hour_count():\n",
    "    print(\"\\t\\t总数\\t已挂号  医保换号  已退号  窗口退号  无号退款  超时取消\")\n",
    "    for i in range(5, 23):\n",
    "        time_filter = data[(data['订单创建时间'].dt.time >= pd.to_datetime(f'{i}:00:00').time()) &\n",
    "                     (data['订单创建时间'].dt.time <= pd.to_datetime(f'{i+1}:00:00').time())]\n",
    "        counts = time_filter['状态'].value_counts()\n",
    "        print(f\"{i}:00 - {i+1}:00 \\t{len(time_filter)}\\t{counts['已挂号']}\\t{counts['医保换号']}\\t {counts['已退号']}\\t\"\n",
    "              f\"   {counts['窗口退号']}\\t   {counts['无号退款']}\\t    {counts['超时取消']}\\t\")\n",
    "\n",
    "def minute_line(data, hour):\n",
    "    count = []\n",
    "    for i in tqdm(range(0, 59, 5)):\n",
    "        start = str(i)\n",
    "        end = str(i+5)\n",
    "        if len(start) < 2:\n",
    "            start = '0' + start\n",
    "        if len(end) < 2:\n",
    "            end = '0' + end \n",
    "        time_filter = data[(data['订单创建时间'].dt.time >= pd.to_datetime(f'{hour}:{start}:00').time()) &\n",
    "                     (data['订单创建时间'].dt.time <= pd.to_datetime(f'{hour}:{end}:00').time())]\n",
    "        count.append(len(time_filter))\n",
    "    plt.plot(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 恰好在16:00进行第二天/下一周操作\n",
    "def hurry_sixteen(data, get_row=False):\n",
    "    gap = daily_filter(data, '16:00:00', '16:00:01')\n",
    "    time_diff = (gap['就诊日期'] - gap['订单创建时间']).dt.days\n",
    "    hurry_row = gap[(time_diff == 0) | (time_diff == 6)]\n",
    "    if get_row:\n",
    "        return hurry_row\n",
    "    else:\n",
    "        return hurry_row['ID'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将之前一次答案中的数据提取成列表\n",
    "def get_list(res_id):\n",
    "    file_path = f'./data/result{res_id}.txt'\n",
    "    with open(file_path, 'r') as file:\n",
    "        lis = [int(line.strip()) for line in file]\n",
    "    return lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 两个list进行对比\n",
    "def lis_cmp(lis1, lis2, ret=False, show=False):\n",
    "    set1 = set(lis1)\n",
    "    set2 = set(lis2)\n",
    "    new_ele = set2 - set1\n",
    "    miss_ele = set1 - set2\n",
    "    same_ele = set1.intersection(set2)\n",
    "    if show:\n",
    "        print(f\"lis1: {len(lis1)},\\tlis2: {len(lis2)},\\tmore: {len(new_ele)},\"\n",
    "            f\"\\tmiss: {len(miss_ele)},\\tsame = {len(same_ele)}\")\n",
    "    if ret:\n",
    "        return list(new_ele), list(miss_ele), list(same_ele)\n",
    "\n",
    "# 与之前一次答案进行对比\n",
    "def res_cmp(res_id, lis, ret=False, show=False):\n",
    "    lis1 = get_list(res_id)\n",
    "    res = lis_cmp(lis1, lis, ret, show)\n",
    "    if ret:\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 频繁退号\n",
    "def frequent_drop(data, limit, get_row=False):\n",
    "    drop = data[data['状态'] == '已退号']\n",
    "    mass_drop = duplicate_detect(drop, \"患者ID\", limit)\n",
    "    if get_row:\n",
    "        return mass_drop['ID'].tolist()\n",
    "    else:\n",
    "        return mass_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不是黄牛的用户\n",
    "def normal_user(data):\n",
    "    # 黑名单\n",
    "    ip_black = duplicate_detect(data, 'IP_ADDRESS', 2)['IP_ADDRESS'].to_list()\n",
    "    user_black = duplicate_detect(data, '患者ID', 4)['患者ID'].to_list()\n",
    "    # 删除黑名单中对应的所有ip和患者ID\n",
    "    white_rows = data[~data['IP_ADDRESS'].isin(ip_black)]\n",
    "    white_rows = white_rows[~data['患者ID'].isin(user_black)]\n",
    "    return white_rows\n",
    "    # 基于用户分组的进一步筛选\n",
    "    # depart_limit = white_rows.groupby('患者ID').filter(lambda group: group['就诊科室名称'].nunique() <= 2)\n",
    "    # app_limit = depart_limit.groupby('患者ID').filter(lambda group: group['APPID'].nunique() <= 2)\n",
    "    # area_limit = app_limit[(app_limit['省份'] == '北京') | (app_limit['省份'] == '河北')]\n",
    "    # time_limit = area_limit[((area_limit['订单创建时间'].dt.time >= pd.to_datetime(f'6:00:00').time()) &\n",
    "    #                  (area_limit['订单创建时间'].dt.time <= pd.to_datetime(f'16:00:00').time())) | \n",
    "    #                  (area_limit['订单创建时间'].dt.time >= pd.to_datetime(f'17:00:00').time())]\n",
    "    # return time_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据权重在多个不同的list中进行合并筛选\n",
    "def weighted_selection(lists, weights, n, limit=0):\n",
    "    # 创建一个字典来存储元素的总权重\n",
    "    total_weights = defaultdict(float)\n",
    "\n",
    "    # 遍历每个列表及其对应的权重\n",
    "    for lst, weight in zip(lists, weights):\n",
    "        for element in lst:\n",
    "            total_weights[element] += weight  # 累加权重\n",
    "\n",
    "    # 将字典转换为列表，并筛选出权重大于 limit 的元素\n",
    "    filtered_elements = {k: v for k, v in total_weights.items() if v >= limit}\n",
    "\n",
    "    # 按照权重排序\n",
    "    sorted_elements = sorted(filtered_elements.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # 选择前 n 个元素及其权重\n",
    "    top_n_elements = sorted_elements[:n]\n",
    "\n",
    "    return top_n_elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 基于聚类的分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ip_encoder(ip):\n",
    "    parts = list(map(int, ip.split('.')))\n",
    "    return (parts[0] << 24) + (parts[1] << 16) + (parts[2] << 8) + parts[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 基于学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state2num = {\n",
    "    '已退号': 0,\n",
    "    '已挂号': 1,\n",
    "    '窗口退号': 2,\n",
    "    '医保换号': 3,\n",
    "    '无号退款': 4,\n",
    "    '超时取消': 5,\n",
    "    '主动取消': 6,\n",
    "    np.nan: 7\n",
    "}\n",
    "\n",
    "def get_embed_list(state_list_pd, seg_name, series_name):\n",
    "    state_lists = state_list_pd.groupby(seg_name)[series_name].apply(list).reset_index()\n",
    "    state_lists = state_lists[series_name].to_list()\n",
    "    for i in range(len(state_lists)):\n",
    "        state_lists[i] = [str(state2num[str_]) for str_ in state_lists[i]]\n",
    "    return state_lists\n",
    "\n",
    "def generate_train_test(data, seg_name, series_name, normal_func, limit=2000, path='../LogBERT/output/rd/'):\n",
    "    # train\n",
    "    normal_rows = normal_func(data)\n",
    "    normal_id = normal_rows[seg_name].to_list()\n",
    "    sample_id = random.sample(normal_id, limit)\n",
    "    normals = data[data[seg_name].isin(sample_id)]\n",
    "    state_lists = get_embed_list(normals, seg_name, series_name)\n",
    "    with open(path + \"train\", 'w') as train_file:\n",
    "        for states in state_lists:\n",
    "            train_file.write(\" \".join(states) + \"\\n\")\n",
    "\n",
    "    # test_normal\n",
    "    unsample_id = list(set(normal_id) - set(sample_id))\n",
    "    test_normals = data[data[seg_name].isin(unsample_id)]\n",
    "    state_lists = get_embed_list(test_normals, seg_name, series_name)\n",
    "    with open(path + \"test_normal\", 'w') as train_file:\n",
    "        for states in state_lists:\n",
    "            train_file.write(\" \".join(states) + \"\\n\")\n",
    "\n",
    "    # test_abnormal\n",
    "    res_id = get_list(5)\n",
    "    test_abnormals = data[data['ID'].isin(res_id)]\n",
    "    state_lists = get_embed_list(test_abnormals, seg_name, series_name)\n",
    "    with open(path + \"test_abnormal\", 'w') as train_file:\n",
    "        for states in state_lists:\n",
    "            train_file.write(\" \".join(states) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LogBERT 魔改记录\n",
    "\n",
    "1. logbert.py里的options[\"min_len\"]改成了1, epoch也改成了2\n",
    "2. sample.py中line = line.squeeze()改为line.ravel()\n",
    "3. sample.py中logkey_seq_pairs = np.array(logkey_seq_pairs,)加上了dtype=object，predict_log.py中如果报错也添加这个就行\n",
    "4. train_log.py中epoch > 0及生成center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 工作区"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\杨智雄\\AppData\\Local\\Temp\\ipykernel_22640\\861378386.py:8: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  white_rows = white_rows[~data['患者ID'].isin(user_black)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3210 3210\n"
     ]
    }
   ],
   "source": [
    "generate_train_test(data, \"患者ID\", \"状态\", normal_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 流放地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# # 特征选择\n",
    "# used_data = pd.DataFrame()\n",
    "# used_data['ID'] = data['ID']\n",
    "# used_data['start_t'] = data['订单创建时间'].dt.hour * 3600 + data['订单创建时间'].dt.minute * 60 + data['订单创建时间'].dt.second\n",
    "# used_data['delta_t'] = (data['就诊日期'] - data['订单创建时间']).astype('int64') / 10**9\n",
    "# used_data['pid'] = data['患者ID']\n",
    "\n",
    "# # 统计每个患者ID对应的不同APPID数量\n",
    "# app_num = data.groupby('患者ID')['APPID'].nunique().reset_index()\n",
    "# app_num.columns = ['pid', 'app_num']  # 重命名列\n",
    "# used_data = used_data.merge(app_num, on='pid', how='left')\n",
    "\n",
    "# # 每个患者的科室挂号数\n",
    "# dorm_num = data.groupby('患者ID')['就诊科室名称'].nunique().reset_index()\n",
    "# dorm_num.columns = ['pid', 'dorm_num']  # 重命名列\n",
    "# used_data = used_data.merge(dorm_num, on='pid', how='left')\n",
    "\n",
    "# used_data['province'] = data['省份']\n",
    "# used_data['ip'] = data['IP_ADDRESS'].apply(ip_encoder)\n",
    "# used_data['status'] = data['状态']\n",
    "\n",
    "# features = ['start_t', 'delta_t', 'pid', 'province', 'ip', 'status', 'app_num']\n",
    "\n",
    "# # 归一化\n",
    "# scaler = MinMaxScaler()  # 或使用 StandardScaler()\n",
    "# used_data[['start_t', 'delta_t', 'ip']] = scaler.fit_transform(used_data[['start_t', 'delta_t', 'ip']])\n",
    "\n",
    "# # 使用 ColumnTransformer 进行特征处理\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', SimpleImputer(strategy='constant'), ['delta_t', 'start_t', 'ip', 'app_num']),\n",
    "#         ('cat', OneHotEncoder(), ['pid', 'province', 'status'])  # 分类特征处理\n",
    "#     ],\n",
    "#     remainder='drop'\n",
    "# )\n",
    "# # 创建聚类管道\n",
    "# pipeline = Pipeline(steps=[\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('kmeans', KMeans(n_clusters=2, random_state=0))\n",
    "# ])\n",
    "\n",
    "# # 拟合模型\n",
    "# pipeline.fit(used_data[features])\n",
    "\n",
    "# # 获取聚类标签\n",
    "# used_data[f'cluster'] = pipeline.predict(used_data[features])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
