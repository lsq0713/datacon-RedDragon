{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 黄牛检测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "file_path = './data/raw_data.xlsx'\n",
    "sheet_name = 'Sheet1'\n",
    "data = pd.read_excel(file_path, sheet_name=sheet_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 时间预处理\n",
    "data['订单创建时间'] = pd.to_datetime(data['订单创建时间'])\n",
    "data['就诊日期'] = pd.to_datetime(data['就诊日期'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ID', '省份', 'APPID', 'IP_ADDRESS', '订单创建时间', '患者ID', '患者创建时间', '就诊日期',\n",
      "       '就诊科室名称', '医生姓名', '状态', '商户订单号'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 写入到指定log文件中(LogBERT相关,已废弃)\n",
    "def write_log(log_file_path):\n",
    "    # 将数据逐行写入.log 文件\n",
    "    with open(log_file_path, 'w', encoding='utf-8') as log_file:\n",
    "        for index, row in data.iterrows():\n",
    "            # 将每一行转换为字符串并写入日志文件\n",
    "            row = row.to_dict()\n",
    "            log_file.write(row['患者ID'] + \" : \")\n",
    "            for k, v in row.items():\n",
    "                if k != \"患者ID\":\n",
    "                    log_file.write(str(v) + \" \")\n",
    "            log_file.write(\"\\n\")\n",
    "    print(f\"Data has been written to {log_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 基于规则的检测\n",
    "\n",
    "1. IP重复，来自同一个IP，且为超过3个人挂号\n",
    "2. 用户重复，来自同一个用户，且挂号了超过3个科室/超过两个app_id\n",
    "3. 时间过早，每天5:00-5:01进行操作的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重复值筛选，找出data的seg_name字段中重复数大于limit的行, sort_add和asc_add是检测完之后添加的排序要求\n",
    "def duplicate_detect(data, seg_name, limit, up=True, sort_add=[], asc_add=[]):\n",
    "    assert len(sort_add) == len(asc_add)\n",
    "    value_counts = data[seg_name].value_counts()\n",
    "    if up:\n",
    "        # 向上筛\n",
    "        dup_row = data[data[seg_name].isin(value_counts[value_counts > limit].index)]\n",
    "    else:\n",
    "        # 向下筛\n",
    "        dup_row = data[data[seg_name].isin(value_counts[value_counts < limit].index)]\n",
    "    sort_by = [seg_name] + sort_add\n",
    "    asc = [True] + asc_add\n",
    "    dup_row = dup_row.sort_values(by=sort_by, ascending=asc)\n",
    "    return dup_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 筛选出同一个dup_seg中有超过limit个unique_seg的ID字段\n",
    "def unique_dup_filter(data, dup_seg, unique_seg, limit, up=True, get_row=False):\n",
    "    # 确定是黄牛的行编号\n",
    "    selects = []\n",
    "    # IP重复检测\n",
    "    dup = duplicate_detect(data, dup_seg, limit, up=up, sort_add=[unique_seg], asc_add=[True])\n",
    "    print(\"init dup_num:\", len(dup))\n",
    "    if len(dup) == 0:\n",
    "        return selects\n",
    "    pos = 0          # 这个IP的起点\n",
    "    count = 1      # 涉及多少个用户ID\n",
    "    last = dup[unique_seg].iloc[0] # 上一个用户的ID\n",
    "    dup_num = len(dup)\n",
    "    for i in tqdm(range(1, dup_num)):\n",
    "        if dup[dup_seg].iloc[i] == dup[dup_seg].iloc[pos]:\n",
    "            if dup[unique_seg].iloc[i] != last:\n",
    "                # 相同IP下一个新的患者\n",
    "                count += 1\n",
    "                last = dup[unique_seg].iloc[i]\n",
    "        if dup[dup_seg].iloc[i] != dup[dup_seg].iloc[pos] or i == dup_num - 1:\n",
    "            # 开始检测下一个IP\n",
    "            if count > limit:\n",
    "                # 达到重复人数条件\n",
    "                for j in range(pos, i):\n",
    "                    selects.append(dup['ID'].iloc[j])\n",
    "            # 重置\n",
    "            pos = i\n",
    "            count = 1\n",
    "            last = dup[unique_seg].iloc[i]\n",
    "    print(\"filtered dup_num:\", len(selects))\n",
    "    if not get_row:\n",
    "        selects.sort()\n",
    "        return selects\n",
    "    else:\n",
    "        dup_rows = data[data['ID'].isin(selects)]\n",
    "        dup_rows = dup_rows.sort_values(by=dup_seg, ascending=True)\n",
    "        return dup_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对unique_dup_filter, 在一定范围内遍历limit(基本不用)\n",
    "# 使用: limits, select_list = grid_traverse(data, '患者ID', '就诊科室名称', 5, 10)\n",
    "def grid_traverse(data, dup_seg, unique_seg, start, end, gap=1):\n",
    "    limits = []\n",
    "    selects_list = []\n",
    "    for limit in range(start, end, gap):\n",
    "        print(f\"limit: {limit}\")\n",
    "        selects = unique_dup_filter(data, dup_seg, unique_seg, limit)\n",
    "        limits.append(limit)\n",
    "        selects_list.append(selects)\n",
    "    return limits, selects_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 时间段过滤器，过滤出每天一段时间内的数据\n",
    "# 形如daily_filter(data, '5:00:00', '5:01:00')\n",
    "def daily_filter(data, start, end):\n",
    "    return data[(data['订单创建时间'].dt.time >= pd.to_datetime(start).time()) &\n",
    "                    (data['订单创建时间'].dt.time <= pd.to_datetime(end).time())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 时间分段统计(数据分析而非处理, 只会打印出每个小时内各个操作的次数)\n",
    "def hour_count():\n",
    "    print(\"\\t\\t总数\\t已挂号  医保换号  已退号  窗口退号  无号退款  超时取消\")\n",
    "    for i in range(5, 23):\n",
    "        time_filter = data[(data['订单创建时间'].dt.time >= pd.to_datetime(f'{i}:00:00').time()) &\n",
    "                     (data['订单创建时间'].dt.time <= pd.to_datetime(f'{i+1}:00:00').time())]\n",
    "        counts = time_filter['状态'].value_counts()\n",
    "        print(f\"{i}:00 - {i+1}:00 \\t{len(time_filter)}\\t{counts['已挂号']}\\t{counts['医保换号']}\\t {counts['已退号']}\\t\"\n",
    "              f\"   {counts['窗口退号']}\\t   {counts['无号退款']}\\t    {counts['超时取消']}\\t\")\n",
    "\n",
    "# 一个小时内按分钟打印操作次数(数据分析而非处理)\n",
    "def minute_line(data, hour):\n",
    "    count = []\n",
    "    for i in tqdm(range(0, 59, 5)):\n",
    "        start = str(i)\n",
    "        end = str(i+5)\n",
    "        if len(start) < 2:\n",
    "            start = '0' + start\n",
    "        if len(end) < 2:\n",
    "            end = '0' + end \n",
    "        time_filter = data[(data['订单创建时间'].dt.time >= pd.to_datetime(f'{hour}:{start}:00').time()) &\n",
    "                     (data['订单创建时间'].dt.time <= pd.to_datetime(f'{hour}:{end}:00').time())]\n",
    "        count.append(len(time_filter))\n",
    "    plt.plot(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 恰好在16:00进行第二天/下一周操作(因为这里面很有可能有很多正常人,不建议使用)\n",
    "def hurry_sixteen(data, get_row=False):\n",
    "    gap = daily_filter(data, '16:00:00', '16:00:01')\n",
    "    time_diff = (gap['就诊日期'] - gap['订单创建时间']).dt.days\n",
    "    hurry_row = gap[(time_diff == 0) | (time_diff == 6)]\n",
    "    if get_row:\n",
    "        return hurry_row\n",
    "    else:\n",
    "        return hurry_row['ID'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将之前一次答案中的数据提取成列表(需要配套data文件夹中的result1/2/3...使用)\n",
    "def get_list(res_id, file_path=\"\"):\n",
    "    if file_path == \"\":\n",
    "        file_path = f'./data/result{res_id}.txt'\n",
    "    with open(file_path, 'r') as file:\n",
    "        lis = [int(line.strip()) for line in file]\n",
    "    return lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 两个list进行对比(相比增加的,减少的,相同的)\n",
    "def lis_cmp(lis1, lis2, ret=False, show=True):\n",
    "    set1 = set(lis1)\n",
    "    set2 = set(lis2)\n",
    "    new_ele = set2 - set1\n",
    "    miss_ele = set1 - set2\n",
    "    same_ele = set1.intersection(set2)\n",
    "    if show:\n",
    "        print(f\"lis1: {len(lis1)},\\tlis2: {len(lis2)},\\tmore: {len(new_ele)},\"\n",
    "            f\"\\tmiss: {len(miss_ele)},\\tsame = {len(same_ele)}\")\n",
    "    if ret:\n",
    "        return list(new_ele), list(miss_ele), list(same_ele)\n",
    "\n",
    "# 与之前一次答案进行对比\n",
    "def res_cmp(res_id, lis, ret=False, show=True):\n",
    "    lis1 = get_list(res_id)\n",
    "    res = lis_cmp(lis1, lis, ret, show)\n",
    "    if ret:\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将一个list写作答案(写到data/result.txt中,remove_white选项是是否去除白名单)\n",
    "def write_list(lis, remove_white=True):\n",
    "    if remove_white:\n",
    "        white_lis = get_list(0, './data/white.txt')\n",
    "        print(f\"remove white: {len(set(lis).intersection(set(white_lis)))}\")\n",
    "        lis = list(set(lis) - set(white_lis))\n",
    "        lis.sort()\n",
    "    # 打开一个文件进行写入，如果文件不存在则创建\n",
    "    with open('./data/result.txt', 'w', encoding='utf-8') as file:\n",
    "        # 遍历列表中的每个元素\n",
    "        for item in lis:\n",
    "            # 将每个元素写入文件，每个元素后面加上换行符\n",
    "            file.write(str(item) + '\\n')\n",
    "        print(f\"Total line: {len(lis)}\")\n",
    "    return lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data中退号超过limit词的(这条规则效果似乎不算太好)\n",
    "def frequent_drop(data, limit, get_row=False):\n",
    "    drop = data[data['状态'] == '已退号']\n",
    "    mass_drop = duplicate_detect(drop, \"患者ID\", limit)\n",
    "    if get_row:\n",
    "        return mass_drop['ID'].tolist()\n",
    "    else:\n",
    "        return mass_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简单筛选不是黄牛的用户(效果不确定)\n",
    "def normal_user(data):\n",
    "    # 黑名单\n",
    "    black_lis = get_list(6)\n",
    "    # 白名单\n",
    "    white_user1 = get_list(-1, file_path=\"./data/white.txt\")\n",
    "    black_rows = data[data['ID'].isin(black_lis)]\n",
    "    black_user = black_rows['患者ID'].to_list()\n",
    "    not_black = data[~data['患者ID'].isin(black_user)]['患者ID'].to_list()\n",
    "    white_sample = random.sample(not_black, 10 * len(white_user1))\n",
    "    white_user2 = data[data['患者ID'].isin(white_sample)]['ID'].to_list()\n",
    "    white_rows1 = data[data['ID'].isin(white_user1)]\n",
    "    white_rows2 = data[data['ID'].isin(white_user2)]\n",
    "    return white_rows1, white_rows2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据权重在多个不同的list中进行合并筛选\n",
    "def weighted_selection(lists, weights, n, limit=0):\n",
    "    # 创建一个字典来存储元素的总权重\n",
    "    total_weights = defaultdict(float)\n",
    "\n",
    "    # 遍历每个列表及其对应的权重\n",
    "    for lst, weight in zip(lists, weights):\n",
    "        for element in lst:\n",
    "            total_weights[element] += weight  # 累加权重\n",
    "\n",
    "    # 将字典转换为列表，并筛选出权重大于 limit 的元素\n",
    "    filtered_elements = {k: v for k, v in total_weights.items() if v >= limit}\n",
    "\n",
    "    # 按照权重排序\n",
    "    sorted_elements = sorted(filtered_elements.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # 选择前 n 个元素及其权重\n",
    "    top_n_elements = sorted_elements[:n]\n",
    "\n",
    "    return top_n_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正常人过滤器\n",
    "# 只要在时间范围内有一次(\"已挂号\")\n",
    "# 返回剩下的可疑数据\n",
    "def normal_filter(data, rows):\n",
    "    all_users = rows['患者ID'].to_list()\n",
    "    normal_users = data[(data['就诊日期'] <= pd.to_datetime('2024-4-9')) & (data['状态'] == \"已挂号\")]['患者ID'].to_list()\n",
    "    abnormal_users = list(set(all_users) - set(normal_users))\n",
    "    filtered_users = list(set(all_users).intersection(set(normal_users)))\n",
    "    abnormal_rows = data[data['患者ID'].isin(abnormal_users)]\n",
    "    normal_rows = data[data['患者ID'].isin(filtered_users)]\n",
    "    return abnormal_rows, normal_rows\n",
    "\n",
    "# 基础过滤器\n",
    "# 去除那些正常的就诊记录\n",
    "def basic_filter(data, select):\n",
    "    copy = list(select)\n",
    "    select_rows = data[data['ID'].isin(select)]\n",
    "    normal_ids = select_rows[(select_rows['就诊日期'] <= pd.to_datetime('2024-4-9')) & (select_rows['状态'] == \"已挂号\")]['ID'].to_list()\n",
    "    copy = list(set(copy) - set(normal_ids))\n",
    "    return copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 基于聚类的分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ip_encoder(ip):\n",
    "    parts = list(map(int, ip.split('.')))\n",
    "    return (parts[0] << 24) + (parts[1] << 16) + (parts[2] << 8) + parts[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 特征选择\n",
    "used_data = pd.DataFrame()\n",
    "used_data['ID'] = data['ID']\n",
    "used_data['start_t'] = data['订单创建时间'].dt.hour * 3600 + data['订单创建时间'].dt.minute * 60 + data['订单创建时间'].dt.second\n",
    "used_data['delta_t'] = (data['就诊日期'] - data['订单创建时间']).astype('int64') / 10**9\n",
    "used_data['pid'] = data['患者ID']\n",
    "\n",
    "# 统计每个患者ID对应的不同APPID数量\n",
    "app_num = data.groupby('患者ID')['APPID'].nunique().reset_index()\n",
    "app_num.columns = ['pid', 'app_num']  # 重命名列\n",
    "used_data = used_data.merge(app_num, on='pid', how='left')\n",
    "\n",
    "# 每个患者的科室挂号数\n",
    "dorm_num = data.groupby('患者ID')['就诊科室名称'].nunique().reset_index()\n",
    "dorm_num.columns = ['pid', 'dorm_num']  # 重命名列\n",
    "used_data = used_data.merge(dorm_num, on='pid', how='left')\n",
    "\n",
    "used_data['province'] = data['省份']\n",
    "used_data['ip'] = data['IP_ADDRESS'].apply(ip_encoder)\n",
    "used_data['status'] = data['状态']\n",
    "\n",
    "features = ['start_t', 'delta_t', 'pid', 'province', 'ip', 'status', 'app_num']\n",
    "\n",
    "# 归一化\n",
    "scaler = MinMaxScaler()  # 或使用 StandardScaler()\n",
    "used_data[['start_t', 'delta_t', 'ip']] = scaler.fit_transform(used_data[['start_t', 'delta_t', 'ip']])\n",
    "\n",
    "# 使用 ColumnTransformer 进行特征处理\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', SimpleImputer(strategy='constant'), ['delta_t', 'start_t', 'ip', 'app_num']),\n",
    "        ('cat', OneHotEncoder(), ['pid', 'province', 'status'])  # 分类特征处理\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "# 创建聚类管道\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('kmeans', KMeans(n_clusters=2, random_state=0))\n",
    "])\n",
    "\n",
    "# 拟合模型\n",
    "pipeline.fit(used_data[features])\n",
    "\n",
    "# 获取聚类标签\n",
    "used_data[f'cluster_num'] = pipeline.predict(used_data[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 基于学习的分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "state2num = {\n",
    "    '已退号': 0,\n",
    "    '已挂号': 1,\n",
    "    '窗口退号': 2,\n",
    "    '医保换号': 3,\n",
    "    '无号退款': 4,\n",
    "    '超时取消': 5,\n",
    "    '主动取消': 6,\n",
    "    np.nan: 7\n",
    "}\n",
    "\n",
    "def get_embed_list(state_list_pd, seg_name, series_name):\n",
    "    state_lists = state_list_pd.groupby(seg_name)[series_name].apply(list).reset_index()\n",
    "    state_lists = state_lists[series_name].to_list()\n",
    "    for i in range(len(state_lists)):\n",
    "        state_lists[i] = [str(state2num[str_]) for str_ in state_lists[i]]\n",
    "    return state_lists\n",
    "\n",
    "def generate_train_test(data, seg_name, series_name, normal_func, limit=2000, path='../LogBERT/output/rd/'):\n",
    "    # train\n",
    "    normal_rows = normal_func(data)\n",
    "    normal_id = normal_rows[seg_name].to_list()\n",
    "    sample_id = random.sample(normal_id, limit)\n",
    "    normals = data[data[seg_name].isin(sample_id)]\n",
    "    state_lists = get_embed_list(normals, seg_name, series_name)\n",
    "    with open(path + \"train\", 'w') as train_file:\n",
    "        for states in state_lists:\n",
    "            train_file.write(\" \".join(states) + \"\\n\")\n",
    "\n",
    "    # test_normal\n",
    "    unsample_id = list(set(normal_id) - set(sample_id))\n",
    "    test_normals = data[data[seg_name].isin(unsample_id)]\n",
    "    state_lists = get_embed_list(test_normals, seg_name, series_name)\n",
    "    with open(path + \"test_normal\", 'w') as train_file:\n",
    "        for states in state_lists:\n",
    "            train_file.write(\" \".join(states) + \"\\n\")\n",
    "\n",
    "    # test_abnormal\n",
    "    res_id = get_list(5)\n",
    "    test_abnormals = data[data['ID'].isin(res_id)]\n",
    "    state_lists = get_embed_list(test_abnormals, seg_name, series_name)\n",
    "    with open(path + \"test_abnormal\", 'w') as train_file:\n",
    "        for states in state_lists:\n",
    "            train_file.write(\" \".join(states) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LogBERT 魔改记录\n",
    "\n",
    "1. logbert.py里的options[\"min_len\"]改成了1, epoch也改成了2\n",
    "2. sample.py中line = line.squeeze()改为line.ravel()\n",
    "3. sample.py中logkey_seq_pairs = np.array(logkey_seq_pairs,)加上了dtype=object，predict_log.py中如果报错也添加这个就行\n",
    "4. train_log.py中epoch > 0及生成center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 手写神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将一个list写入白名单\n",
    "def add_white(lis, path='./data/white.txt'):\n",
    "    former_list = []\n",
    "    if os.path.isfile(path):\n",
    "        former_list = get_list(0, path)\n",
    "    lis = list(set(lis + former_list))\n",
    "    with open(path, 'w', encoding='utf-8') as file:\n",
    "        for item in lis:\n",
    "            file.write(str(item) + '\\n')\n",
    "        print(f\"Total line: {len(lis)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 省份, 患者ID, 就诊科室名称, 医生姓名, 状态\n",
    "\n",
    "# 按照用户分组提取出操作的时间序列，然后嵌入成一个torch.tensor\n",
    "def feature_extract(rows):\n",
    "    res_list = []\n",
    "    user_ids = []\n",
    "    # 根据User分成dataFrame的list\n",
    "    user_group = rows.groupby('患者ID')\n",
    "    data_list = [group.reset_index(drop=True) for name, group in user_group]\n",
    "    \n",
    "    # 每一组根据时间进行排序\n",
    "    for i in range(len(data_list)):\n",
    "        data_list[i] = data_list[i].sort_values(by='就诊日期', ascending=True)\n",
    "        \n",
    "        prov_tensor = torch.tensor(data_list[i]['prov_id'].values).unsqueeze(0)\n",
    "        depart_tensor = torch.tensor(data_list[i]['depart_id'].values).unsqueeze(0)\n",
    "        doc_tensor = torch.tensor(data_list[i]['doc_id'].values).unsqueeze(0)\n",
    "        state_tensor = torch.tensor(data_list[i]['state_id'].values).unsqueeze(0)\n",
    "        embedding = torch.cat((prov_tensor, depart_tensor, doc_tensor, state_tensor), dim=0).T\n",
    "        res_list.append(embedding)\n",
    "        user_ids.append(data_list[i]['患者ID'][0])\n",
    "    \n",
    "    return res_list, user_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将tensor填充成指定行数\n",
    "def padding(tensor, target_length):\n",
    "    current_length = tensor.size(0)  # 当前行数\n",
    "    padding_amount = target_length - current_length\n",
    "    if padding_amount > 0:\n",
    "        padding_tensor = torch.zeros(padding_amount, tensor.size(1), dtype=tensor.dtype)\n",
    "        padded_tensor = torch.cat((tensor, padding_tensor), dim=0)\n",
    "    else:\n",
    "        # 如果当前行数已经大于或等于目标长度，直接使用原 tensor\n",
    "        padded_tensor = tensor[:target_length]\n",
    "    return padded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_user_statistic(data):\n",
    "    id_counts = data.groupby('患者ID').size()\n",
    "    for i in range(0, 20):\n",
    "        filtered_ids = id_counts[id_counts > i]\n",
    "        result_count = len(filtered_ids)\n",
    "        print(f'操作数大于 {i} 的 患者 个数: {result_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_loader(samples, labels, SequenceDataset, train_ratio):\n",
    "    samples_list = list(samples)\n",
    "    labels_list = list(labels)\n",
    "\n",
    "    # 设置划分比例\n",
    "    train_size = int(train_ratio * len(samples_list))\n",
    "    indices = np.random.permutation(len(samples_list))  # 随机打乱索引\n",
    "\n",
    "    # 划分索引\n",
    "    train_indices = indices[:train_size]\n",
    "    test_indices = indices[train_size:]\n",
    "\n",
    "    # 创建训练和测试数据集\n",
    "    train_samples = [samples_list[i] for i in train_indices]\n",
    "    train_labels = [labels_list[i] for i in train_indices]\n",
    "\n",
    "    test_samples = [samples_list[i] for i in test_indices]\n",
    "    test_labels = [labels_list[i] for i in test_indices]\n",
    "\n",
    "    train_dataset = SequenceDataset(train_samples, train_labels)\n",
    "    test_dataset = SequenceDataset(test_samples, test_labels)\n",
    "\n",
    "    # 创建 DataLoader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "# 先对类别列进行编码\n",
    "data = data.fillna(method='ffill')     # 前值替换缺失值\n",
    "data['prov_id'] = pd.factorize(data['省份'])[0]\n",
    "data['depart_id'] = pd.factorize(data['就诊科室名称'])[0]\n",
    "data['doc_id'] = pd.factorize(data['医生姓名'])[0]\n",
    "data['state_id'] = pd.factorize(data['状态'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备数据\n",
    "## 黄牛\n",
    "abnormal_id = get_list(6)\n",
    "abnormal_rows = data[data['ID'].isin(abnormal_id)]\n",
    "## 正常人\n",
    "normal_rows1, normal_rows2 = normal_user(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 黄牛特征向量\n",
    "abnormal_tensors, _ = feature_extract(abnormal_rows)\n",
    "# 普通人的特征向量\n",
    "normal_tensors1, _ = feature_extract(normal_rows1)\n",
    "normal_tensors2, _ = feature_extract(normal_rows2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_tensors1 = normal_tensors1 * 10\n",
    "normal_tensors = normal_tensors1 + normal_tensors2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建数据集\n",
    "## 标签数据\n",
    "normal_labels = torch.zeros(len(normal_tensors), dtype=torch.long)\n",
    "abnormal_labels = torch.ones(len(abnormal_tensors), dtype=torch.long)\n",
    "samples = normal_tensors + abnormal_tensors\n",
    "# 序列填充\n",
    "for i in range(len(samples)):\n",
    "    samples[i] = padding(samples[i], 15)\n",
    "labels = torch.cat((normal_labels, abnormal_labels))\n",
    "\n",
    "## 数据集\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, samples, labels):\n",
    "        self.samples = samples\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx], self.labels[idx]\n",
    "## 划分训练与测试\n",
    "train_loader, test_loader = train_test_loader(samples, labels, SequenceDataset, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型定义\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, model_dim, num_heads, num_classes, num_layers):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Linear(input_dim, model_dim)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(model_dim, num_heads),\n",
    "            num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(model_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_length, input_dim)\n",
    "        x = self.embedding(x)  # (batch_size, seq_length, model_dim)\n",
    "        x = x.transpose(0, 1)  # (seq_length, batch_size, model_dim) for Transformer\n",
    "        x = self.transformer_encoder(x)  # (seq_length, batch_size, model_dim)\n",
    "        x = x.mean(dim=0)  # Global average pooling\n",
    "        x = self.fc(x)  # (batch_size, num_classes)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Library\\Python\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "# 超参数设置\n",
    "# 超参数\n",
    "input_dim = 4  # 每个时间步的特征数量\n",
    "model_dim = 64  # Transformer 模型维度\n",
    "num_heads = 4  # 注意力头数量\n",
    "num_classes = 2  # 分类数量（正常和异常）\n",
    "num_layers = 2  # Transformer 层数\n",
    "\n",
    "# 创建模型，损失函数和优化器\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "model = TransformerModel(input_dim, model_dim, num_heads, num_classes, num_layers).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Library\\Python\\anaconda\\lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.0754\n",
      "Epoch [2/10], Loss: 0.0568\n",
      "Epoch [3/10], Loss: 0.0854\n",
      "Epoch [4/10], Loss: 0.0641\n",
      "Epoch [5/10], Loss: 0.4483\n",
      "Epoch [6/10], Loss: 0.0041\n",
      "Epoch [7/10], Loss: 0.0125\n",
      "Epoch [8/10], Loss: 0.0699\n",
      "Epoch [9/10], Loss: 0.0671\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m batch_labels \u001b[38;5;241m=\u001b[39m batch_labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 10\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_samples\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 前向传播\u001b[39;00m\n\u001b[0;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_labels)  \u001b[38;5;66;03m# 计算损失\u001b[39;00m\n\u001b[0;32m     12\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# 反向传播\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Library\\Python\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Library\\Python\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[31], line 17\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     15\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)  \u001b[38;5;66;03m# (batch_size, seq_length, model_dim)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (seq_length, batch_size, model_dim) for Transformer\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (seq_length, batch_size, model_dim)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Global average pooling\u001b[39;00m\n\u001b[0;32m     19\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x)  \u001b[38;5;66;03m# (batch_size, num_classes)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Library\\Python\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Library\\Python\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Library\\Python\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:416\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[1;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    413\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 416\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[0;32m    419\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[1;32mc:\\Library\\Python\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Library\\Python\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Library\\Python\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:749\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[1;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    747\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 749\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sa_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    750\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Library\\Python\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:757\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[1;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sa_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor,\n\u001b[0;32m    756\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 757\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    758\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    759\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    760\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    761\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n",
      "File \u001b[1;32mc:\\Library\\Python\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Library\\Python\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Library\\Python\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1275\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   1261\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[0;32m   1262\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[0;32m   1263\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1272\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[0;32m   1273\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[0;32m   1274\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1275\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1277\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1278\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1279\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1284\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[0;32m   1287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[1;32mc:\\Library\\Python\\anaconda\\lib\\site-packages\\torch\\nn\\functional.py:5561\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   5558\u001b[0m v \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mview(bsz, num_heads, src_len, head_dim)\n\u001b[0;32m   5560\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n\u001b[1;32m-> 5561\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mattn_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(bsz \u001b[38;5;241m*\u001b[39m tgt_len, embed_dim)\n\u001b[0;32m   5563\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m linear(attn_output, out_proj_weight, out_proj_bias)\n\u001b[0;32m   5564\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mview(tgt_len, bsz, attn_output\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 模型训练\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_samples, batch_labels in train_loader:\n",
    "        batch_samples = batch_samples.to(device).float()\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_samples)  # 前向传播\n",
    "        loss = criterion(outputs, batch_labels)  # 计算损失\n",
    "        loss.backward()  # 反向传播\n",
    "        optimizer.step()  # 优化器更新\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 94.52%\n"
     ]
    }
   ],
   "source": [
    "# 模型评估\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_samples, batch_labels in test_loader:\n",
    "        batch_samples = batch_samples.to(device).float()\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        outputs = model(batch_samples)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += batch_labels.size(0)\n",
    "        correct += (predicted == batch_labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实际使用\n",
    "model.eval()\n",
    "all_tensors, users = feature_extract(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225429/225429 [03:43<00:00, 1008.36it/s]\n"
     ]
    }
   ],
   "source": [
    "classify = []\n",
    "for i in tqdm(range(len(all_tensors))):\n",
    "    all_tensors[i] = padding(all_tensors[i], 15)\n",
    "    input = all_tensors[i].to(device).float().unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        output = model(input)\n",
    "    predicted_class = torch.argmax(output, dim=1)\n",
    "    classify.append(predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [i for i, value in enumerate(classify) if value == 1]\n",
    "abnormal_uids = [users[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有limit次正常挂号以上\n",
    "def sign_up_ids(data, limit):\n",
    "    normals = data[(data['就诊日期'] <= pd.to_datetime('2024-4-9')) & (data['状态'] == \"已挂号\")]\n",
    "    normals = duplicate_detect(normals, '患者ID', limit)\n",
    "    normal_user = normals['患者ID'].to_list()\n",
    "    normal_ids = data[data['患者ID'].isin(normal_user)]['ID'].to_list()\n",
    "    return normal_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 工作区"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据分析结论\n",
    "\n",
    "1. 从第一次提交到第3次，涨幅均是来自于发现了科室限制规则中的其他数据，也就是说，其间新增的规则无效\n",
    "2. 基于用户-科室的筛选的作用已经充分利用，继续基于这条规则展开没有意义\n",
    "3. 一个黄牛用户可能并不总是黄牛，必须去掉答案中那些已正常就诊的记录"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "手动发现:\n",
    "1. T001074\n",
    "2. T001519\n",
    "3. T002529\n",
    "4. T056389\n",
    "5. T094377"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
